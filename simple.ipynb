{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lessons from the Machine Learning Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "- AI at Dell (AI COE)\n",
    "- The Hackathon\n",
    "- Algorithms Introduction\n",
    "- Common Challenges\n",
    "    - NLP Pre-processing/Feature Engineering\n",
    "- Their approach\n",
    "- Our approach\n",
    "- Q & A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI at Dell\n",
    "- AI COE\n",
    "- Purpose\n",
    "- Resources\n",
    "- website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackathon\n",
    "- The Product Reviews and Ratings Problem\n",
    "- 2 days\n",
    "- The Resources\n",
    "- Teams and Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Classifiers Introduction or Algorithms\n",
    "- Lots of data split into two or more categories e.g. in your case positive/negative sentiment. \n",
    "- Goal is to learn how the articles are split into those two categories and then be able to classify new articles on it's own.\n",
    "\n",
    "#### Supervised Learning ML Methods\n",
    "- Supervised Learning or prior knowledge of output values for samples\n",
    "- Goal of supervised learning is to learn a function that, given a sample of data and desired outputs, best approximates the relationship between input and output observable in the data\n",
    "- Or given inputs and outputs updates an _internal_state_ then uses this internal state along wiht\n",
    "##### Classification and regression\n",
    "- map input to output labels\n",
    "- map input to continuous output\n",
    "\n",
    "#### Logistic Regression\n",
    "- Linearly separable: The simpliest but can be very effective (March madness prediction winner) \n",
    "- Pre processing required\n",
    "- Discriminative: Tries to figure out differences (negative/postive sentiment) in the examples\n",
    "#### Naive Bayes\n",
    "- Generative Model\n",
    "- Bayes Theorm (Prob Y from Features F is...) \"Probability that an email is spam\" given strong independent condition features\n",
    "- Pre processing required\n",
    "- Generative Model \"learn how the data was generated\" ...what is the distribution\n",
    "#### SVM\n",
    "- Noise and outliers\n",
    "#### RNN\n",
    "- Discriminative\n",
    "- Deep Learning Algorithm\n",
    "\n",
    "###\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "### Artificial Neuron\n",
    "![Perceptron](img/perceptron.png)\n",
    "- Perceptron Model; 3 inputs and binary output or “activation”\n",
    "- Each input is assigned a WEIGHT (Black Arrow) or influence it has over the OUTPUT\n",
    "\n",
    "- Determine activation = weighted sum of each inputs then determine if it’s <= or >= threshold\n",
    "- Threshold or bias (b)\n",
    "- 0 or 1\n",
    "- Summation is x¹w¹ + x²w² + x³w³ or Dot product is (w * x)\n",
    "- So f(x)= \"1\" if (w*x + b) (or 0 otherwise)\n",
    "\n",
    "\n",
    "#### Example \"Pizza\"\n",
    "- Decision: \n",
    "  -- Make pizza for dinner?\n",
    "- Inputs:\n",
    "  -- x¹ = 1 # We have all of the ingredients.\n",
    "  -- x² = 0 # I'm not in the mood for pizza.\n",
    "  -- x³ = 1 # My partner is in the mood for pizza.\n",
    "- Weights:\n",
    "  -- w¹ = 3 # Having the ingredients makes me willing to have pizza.\n",
    "  -- w² = 4 # If I want pizza, I really want pizza!\n",
    "  -- w³ = 2 # Her wanting pizza is the least of my concerns.\n",
    "- Bias: \n",
    "  -- 4\n",
    "\n",
    "\n",
    "(1*3) + (0*4) + (1*2) = 5 <= 4 #=> FALSE\n",
    "(1*3) + (0*4) + (1*2) = 5 <= 4 #=> TRUE = 1\n",
    "\n",
    "- Now ask the question, is this accurate?  If not we can adjust the Weights until it becomes so...\n",
    "- We “train” a network by giving it inputs and expected outputs \n",
    "- Adjust the weights and biases to converge on desired output\n",
    "- After we train our network, we give it new test data (not seen) \n",
    "- If w and b are calibrated well, patterns emerge and so predictions are accurate\n",
    "\n",
    "#### Loss\n",
    "- Given that our neuron produces an output, and we know what we expect the output to be, we can define how well our neuron is doing like this:\n",
    "- expected_output - actual_output\n",
    "- And we use this as input for adjusting weights\n",
    "- Loss function can be sum of squares of the absolute errors (performance metric)\n",
    "\n",
    "#### Differentiation\n",
    "- Optimise the weights called differentiation or derivative of the loss function\n",
    "- Process that looks like gravity on a ball in a graph of a curve...gradient decesent \n",
    "- However this only considers a \"one-layer\" neural network\n",
    "- More layers are needed to reach more variation in a NN\n",
    "\n",
    " \n",
    "#### Back-propation (Nick)\n",
    "\n",
    "### Genetic Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Challenges\n",
    "\n",
    "### Pre-Processing or \"NLTK\" (Cleaning, Formatting, Scaling, and Normalization)\n",
    "- NLTK Stop Work Removal\n",
    "- Removing Punctuations\n",
    "- Common Words Removal\n",
    "- Rare Words Removal\n",
    "- Spell Correction\n",
    "- Stemming\n",
    "- Lemmatization\t\n",
    "- POS Tagging\n",
    "- Name-Entity Recognition\n",
    "- How large is the data set (1.6 million rows?)\n",
    "- Is it \"balanced\"; Positive versus Neutral versus other categories;  Random under and over sampling or Stratification\n",
    "\n",
    "\n",
    "### Examples:\n",
    "- \tRemoving unnecessary punctuation, tags, special charcters\n",
    "- \tRemoving stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic meaning\n",
    "-   Removing common words -- \"hard drive\" in Storage Product reviews\n",
    "-   Spelling correction\n",
    "\n",
    "- Ex: Top 50 words in db\n",
    "![top50wordsBefore](img/top50palabrasdirty.png)\n",
    "![top50wordsBefore](img/top50palabrasclean.png)\n",
    "\n",
    "- Ex: WordCloud\n",
    "![WordCloud](img/wordcloud1.png)\n",
    "\n",
    "- Ex: Stemming and Lemmatization\n",
    "- Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
    "-  Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language.\n",
    "\n",
    "- The stemmed form of studies is: studi\n",
    "- The stemmed form of studying is: study\n",
    "- The lemmatized form of studies is: study\n",
    "- The lemmatized form of studying is: study\n",
    "\n",
    "- Resource Limitations\n",
    "-- Memory\n",
    "-- Processing (GPU Acceleration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "from __future__ import print_function\n",
    "import neat\n",
    "import visualize\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU,CuDNNLSTM,CuDNNGRU, Reshape, MaxPooling1D,AveragePooling1D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Program Parameters\n",
    "TRAIN_DATA_FILE = \"train.csv\"\n",
    "ga_config = \"ga_config.txt\" # Parameters for the genetic algorithm\n",
    "max_features = 250000 # Maximum Number of Words in Dictionary\n",
    "maxlen = 300   # Maximum Sequence Size\n",
    "\n",
    "# Genetic Algorithm Hyperparameters\n",
    "max_generations = 100 # previously 300\n",
    "limit = 100000 # limit number of rows to train per network\n",
    "# Note: limit must match fitness_threshold in ga_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing\n",
    "-- How large is the data set (1.6 million rows?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data from train.csv\n",
      ".\n",
      ".\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Files\n",
    "print (\"Loading Data from \" + str(TRAIN_DATA_FILE))\n",
    "final_validation_file = pd.read_csv(TRAIN_DATA_FILE)[:176230]  # (176230)\n",
    "print(\".\")\n",
    "test_df = pd.read_csv(TRAIN_DATA_FILE)[176231:511230] # (335000)\n",
    "print(\".\")\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)[511231:1672297]\n",
    "print(\"done!\")\n",
    "total_rows = len(train_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               ReviewText  Rating\n",
      "511231  Only had it for 2 weeks, but so far it seems f...       4\n",
      "511232  Great to deal with this company.  Turned out m...       5\n",
      "511233  It does the job for a very low price what more...       5\n",
      "511234  My brother will be very happy, I gave him my H...       5\n",
      "511235  I do not subject my battery to a lot of wear a...       2\n",
      "Total rows:  1161066\n"
     ]
    }
   ],
   "source": [
    "# Data preview\n",
    "print(train_df.head())\n",
    "print(\"Total rows: \", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format Data For Batch Processing\n",
      ".\n",
      ".\n",
      ".\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Format Data For Batch Processing\")\n",
    "print(\".\")\n",
    "list_sentences_train = train_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "list_classes = [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\n",
    "num_classes=5\n",
    "print(\".\")\n",
    "#y = train_df[list_classes].values\n",
    "target=train_df['Rating'].values\n",
    "y1=to_categorical(target)\n",
    "y=np.delete(y1, 0, axis=1)\n",
    "print(\".\")\n",
    "list_sentences_test = test_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "yaux=y[:,[0]]\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "1. Re-Sampling for Class Balancing\n",
    "1. Domain features with TF-IDF\n",
    "2. N-Gram Bag of Words\n",
    "3. TF-IDF Word Features\t\n",
    "4. Word Embeddings\n",
    "Replace words with numbers:\n",
    "1. Bag of Words (BOW)\n",
    "2. TF-IDF\n",
    "In text processing, words of the text represent discrete, categorical features. How do we encode such data in a way which is ready to be used by the algorithms? The mapping from textual data to real valued vectors is called feature extraction. One of the simplest techniques to numerically represent text is Bag of Words\n",
    "BOW - list of unique words, and represent each sentence/doc as vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training sentences into arrays of words \n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split test sentences into arrays of words \n",
    "\n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create an index and summary of words in word arrays\n",
      "Convert arrays of words into bag-of-word arrays\n"
     ]
    }
   ],
   "source": [
    "print(\"Create an index and summary of words in word arrays\")\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "print(\"Convert arrays of words into bag-of-word arrays\")\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a summary of data\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Average train sequence length: 119\n",
      "Average test sequence length: 118\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Max train sequence length: 6344\n",
      "Max test sequence length: 5626\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Min train sequence length: 0\n",
      "Min test sequence length: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Print a summary of data\")\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, sequences)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, test_sequences)), dtype=int)))\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Max train sequence length: {}'.format(np.max(list(map(len, sequences)))))\n",
    "print('Max test sequence length: {}'.format(np.max(list(map(len, test_sequences)))))\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Min train sequence length: {}'.format(np.min(list(map(len, sequences)))))\n",
    "print('Min test sequence length: {}'.format(np.min(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 508584 unique tokens\n",
      "Shape of data tensor: (1161066, 300)\n",
      "Shape of label tensor: (1161066, 5)\n",
      "Shape of test_data tensor: (334999, 300)\n"
     ]
    }
   ],
   "source": [
    "# Name word index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "# Format data so that it'll fit into the neural network\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example conversion of text to padded Sequence:\n",
      "Only had it for 2 weeks, but so far it seems functional. While editing photos (pushing the computer hard) the battery lasts 2.5'ish hours.Placed it in a Dv6 1030 laptop.\n",
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0     56     42\n",
      "      6     10     71    579     18     22    161      6    206   1371\n",
      "    128   1577    454   2366      1    126    150      1    109   1520\n",
      "     71 205189    337   1257      6     12      5  10586  18823    143]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example conversion of text to padded Sequence:\")\n",
    "print(comments[0])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Their Approach\n",
    "\n",
    "\n",
    "- Modeling Techniques\t\n",
    "1. Logistic Regression \n",
    "2. Naïve Bayes\n",
    "3. Random Forest\n",
    "4. LDA\n",
    "5. SVM\n",
    "6. XGBoost\n",
    "7. Neural Networks (RNN-LSTM)\n",
    " - Calculate Loss: MSE\n",
    " - Write as multivariable function\n",
    "     - This system of calculating partial derivatives by working backwards is known as backpropagation, or “backprop”.\n",
    " - Train \n",
    "Before we train our network, we first need a way to quantify how “good” it’s doing so that it can try to do “better”. That’s what the loss is.\n",
    "We’ll use the mean squared error (MSE) loss:\n",
    "We now have a clear goal: minimize the loss of the neural network. We know we can change the network’s weights and biases to influence its predictions, but how do we do so in a way that decreases loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Our Approach\n",
    "- NEAT\n",
    "- Comparison to traditional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines the fitness function for a genome as the accuracy of its neural network\n",
    "# Evaluates the fitness by building and testing the evolved neural network\n",
    "def category_match(calculated, correct):\n",
    "    rounded = list(map(round, calculated))\n",
    "    matches = 0\n",
    "    for category in range(0,len(correct)):\n",
    "        if rounded[category] == correct[category]:\n",
    "            matches += 1\n",
    "    return matches/(len(correct)-1)\n",
    "    \n",
    "def eval_genomes(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness = 0 # default fitness (should be very bad)\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        count = 0\n",
    "        matches = 0\n",
    "        for xi, xo in zip(data[:limit], y[:limit]):\n",
    "            output = net.activate(xi)\n",
    "            count += 1\n",
    "            matches += category_match(output, xo)\n",
    "        # The fittest individual has the most correct categorizations\n",
    "        genome.fitness = matches/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Runs the genetic algorithm\n",
    "def run_ga(config_file):\n",
    "    # Load configuration.\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                         config_file)\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = neat.Population(config)\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    p.add_reporter(neat.Checkpointer(5))\n",
    "\n",
    "    # Run for up to 300 generations.\n",
    "    winner = p.run(eval_genomes, max_generations)\n",
    "\n",
    "    # Display the winning genome (Very ugly)\n",
    "#     print('\\nBest genome:\\n{!s}'.format(winner))\n",
    "\n",
    "    # Show output of the most fit genome against training data.\n",
    "    #     @TODO: Replace with validation set here\n",
    "    print('\\nMost fit individual performance:')\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    for xi, xo in zip(data[:limit], y[:limit]):\n",
    "        output = winner_net.activate(xi)\n",
    "        print(\"input {!r}, expected output {!r}, got {!r}\".format(xi, xo, output))\n",
    "\n",
    "#     visualize.draw_net(config, winner, True)\n",
    "#     visualize.plot_stats(stats, ylog=False, view=True)\n",
    "#     visualize.plot_species(stats, view=True)\n",
    "\n",
    "#     p = neat.Checkpointer.restore_checkpoint('neat-checkpoint-4')\n",
    "    p.run(eval_genomes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_generations 100\n",
      "\n",
      " ****** Running generation 0 ****** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"max_generations\",max_generations)\n",
    "run_ga(ga_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting Tangents (or Gradient Descents?)\n",
    "## Applications\n",
    "#### Autonomus vechicles\n",
    "- SVM Used for Traffic Lights\n",
    "## Ethics\n",
    "- Future Criminal prediction (Machine Bias: propublica.org)\n",
    "- Predicting pregancy before you or your doctor knows (google)\n",
    "- Using your past posted pictures to determine mental health and \"adjust\" insurance rates\n",
    "- Who to blame?  Original Design \"Trolley Problem\" (google)\n",
    "- Unsettled law (https://arxiv.org/abs/1802.07782)\n",
    "- Buiness Dangers in using AI https://www.cio.com/article/3256031/risky-ai-business-navigating-regulatory-and-legal-dangers-to-come.html\n",
    "\n",
    "\n",
    "## Hype\n",
    "https://www.forbes.com/sites/cognitiveworld/2019/04/22/debunking-the-myths-and-reality-of-artificial-intelligence/#571a4c6c43b5\n",
    "https://www.ft.com/content/4367e34e-db72-11e7-9504-59efdb70e12f: \n",
    "\"If we have Elon Musk and [Oxford university’s] Nick Bostrom talking about ‘superintelligence’, we need [sceptics like] Gary Marcus to provide a reality check.”\n",
    "https://futurism.com/artificial-intelligence-hype\n",
    "https://www.wired.com/story/the-life-threatening-consequences-of-overhyping-ai/:\n",
    "\"There, we see the AI was just 83.7 percent accurate, while the physician accuracies were all above 95 percent. In other words, the human doctors beat the AI system when it came to correctly diagnosing a more serious illness. The reporter doesn’t reference this point in the analysis, but I feel it’s vital to include this detail for consideration\"\n",
    "\"Also, the AI may be cheating. AI is famous for “cheating the test,” like how it beats certain games by exploiting bugs. \"\n",
    "https://www.forbes.com/sites/petercohan/2019/02/15/3-reasons-ai-is-way-overhyped/#60af35f65a6a\n",
    "https://www.fast.ai/2018/07/23/auto-ml-3/ (read \"Then why all the hype about Google's ML\")\n",
    "-- One example of Google’s misleading coverage of its own achievements occurred when Google AI researchers released “a deep learning technology to reconstruct the true human genome”, compared their own work to Nobel prize-winning discoveries (the hubris!), and the story was picked up by Wired. However, Steven Salzberg, a distinguished professor of Biomedical Engineering, Computer Science, and Biostatistics at Johns Hopkins University debunked Google’s post. Salzberg pointed out that the research didn’t actually reconstruct the human genome and was “little more than an incremental improvement over existing software, and it might be even less than that.”\n",
    "- https://www.forbes.com/sites/cognitiveworld/2018/10/12/artificial-intelligence-needs-to-reset/#426baea5386e\n",
    "\n",
    "\n",
    "## Math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
