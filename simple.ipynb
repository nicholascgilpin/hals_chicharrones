{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lessons from the Machine Learning Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "- AI at Dell (AI COE)\n",
    "- The Hackathon\n",
    "- Algorithms Introduction\n",
    "- Common Challenges\n",
    "    - NLP Pre-processing/Feature Engineering\n",
    "- Their approach\n",
    "- Our approach\n",
    "- Q & A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI at Dell\n",
    "- AI COE\n",
    "- Purpose\n",
    "- Resources\n",
    "- website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackathon\n",
    "- The Product Reviews and Ratings Problem\n",
    "- 2 days\n",
    "- The Resources\n",
    "- Teams and Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Introduction\n",
    "\n",
    "### Neural Networks\n",
    "![image.png](attachment:image.png)\n",
    "- Architectures\n",
    "\n",
    "- Back-propation\n",
    "\n",
    "### Genetic Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Challenges\n",
    "- Pre-Pre-processing\n",
    "-- How large is the data set (1.6 million rows?)\n",
    "-- Is it \"balanced\"; Positive versus Neutral versus other categories;  Random under and over sampling or Stratification\n",
    "\n",
    "- Pre-Processing\t\n",
    "1. NLTK Stop Work Removal\n",
    "2. Removing Punctuations\n",
    "3. Common Words Removal\n",
    "4. Rare Words Removal\n",
    "5. Spell Correction\n",
    "6. Stemming\n",
    "7. Lemmatization\t\n",
    "8. POS Tagging\n",
    "9. Name-Entity Recognition\n",
    "\n",
    "•\tTokenization — convert sentences to words\n",
    "•\tRemoving unnecessary punctuation, tags, special charcters\n",
    "•\tRemoving stop words — frequent words such as ”the”, ”is”, etc. that do not have specific semantic meaning\n",
    "*   Removing common words -- \"hard drive\" in Storage Product reviews\n",
    "*   Spelling correction\n",
    "•\tTop 50 words in db\n",
    "\n",
    "![WordCloud](\"img/wordcloud1.img\")\n",
    "![top50wordsBefore](img/top50palabsdirty.png)\n",
    "![top50wordsBefore](img/top50palabsclean.png)\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "•\tStemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.\n",
    "•\tLemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language.\n",
    "The stemmed form of studies is: studi\n",
    "The stemmed form of studying is: study\n",
    "The lemmatized form of studies is: study\n",
    "The lemmatized form of studying is: study\n",
    "\n",
    "- Resource Limitations\n",
    "1. Memory\n",
    "2. Processing (GPU Acceleration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "from __future__ import print_function\n",
    "import neat\n",
    "import visualize\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU,CuDNNLSTM,CuDNNGRU, Reshape, MaxPooling1D,AveragePooling1D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Program Parameters\n",
    "TRAIN_DATA_FILE = \"train.csv\"\n",
    "ga_config = \"ga_config.txt\" # Parameters for the genetic algorithm\n",
    "max_features = 250000 # Maximum Number of Words in Dictionary\n",
    "maxlen = 300   # Maximum Sequence Size\n",
    "\n",
    "# Genetic Algorithm Hyperparameters\n",
    "max_generations = 100 # previously 300\n",
    "limit = 100000 # limit number of rows to train per network\n",
    "# Note: limit must match fitness_threshold in ga_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data from train.csv\n",
      ".\n",
      ".\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Load Files\n",
    "print (\"Loading Data from \" + str(TRAIN_DATA_FILE))\n",
    "final_validation_file = pd.read_csv(TRAIN_DATA_FILE)[:176230]  # (176230)\n",
    "print(\".\")\n",
    "test_df = pd.read_csv(TRAIN_DATA_FILE)[176231:511230] # (335000)\n",
    "print(\".\")\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)[511231:1672297]\n",
    "print(\"done!\")\n",
    "total_rows = len(train_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               ReviewText  Rating\n",
      "511231  Only had it for 2 weeks, but so far it seems f...       4\n",
      "511232  Great to deal with this company.  Turned out m...       5\n",
      "511233  It does the job for a very low price what more...       5\n",
      "511234  My brother will be very happy, I gave him my H...       5\n",
      "511235  I do not subject my battery to a lot of wear a...       2\n",
      "Total rows:  1161066\n"
     ]
    }
   ],
   "source": [
    "# Data preview\n",
    "print(train_df.head())\n",
    "print(\"Total rows: \", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format Data For Batch Processing\n",
      ".\n",
      ".\n",
      ".\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Format Data For Batch Processing\")\n",
    "print(\".\")\n",
    "list_sentences_train = train_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "list_classes = [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\n",
    "num_classes=5\n",
    "print(\".\")\n",
    "#y = train_df[list_classes].values\n",
    "target=train_df['Rating'].values\n",
    "y1=to_categorical(target)\n",
    "y=np.delete(y1, 0, axis=1)\n",
    "print(\".\")\n",
    "list_sentences_test = test_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "yaux=y[:,[0]]\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "1. Re-Sampling for Class Balancing\n",
    "1. Domain features with TF-IDF\n",
    "2. N-Gram Bag of Words\n",
    "3. TF-IDF Word Features\t\n",
    "4. Word Embeddings\n",
    "Replace words with numbers:\n",
    "1. Bag of Words (BOW)\n",
    "2. TF-IDF\n",
    "In text processing, words of the text represent discrete, categorical features. How do we encode such data in a way which is ready to be used by the algorithms? The mapping from textual data to real valued vectors is called feature extraction. One of the simplest techniques to numerically represent text is Bag of Words\n",
    "BOW - list of unique words, and represent each sentence/doc as vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training sentences into arrays of words \n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split test sentences into arrays of words \n",
    "\n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create an index and summary of words in word arrays\n",
      "Convert arrays of words into bag-of-word arrays\n"
     ]
    }
   ],
   "source": [
    "print(\"Create an index and summary of words in word arrays\")\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "print(\"Convert arrays of words into bag-of-word arrays\")\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a summary of data\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Average train sequence length: 119\n",
      "Average test sequence length: 118\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Max train sequence length: 6344\n",
      "Max test sequence length: 5626\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Min train sequence length: 0\n",
      "Min test sequence length: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Print a summary of data\")\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, sequences)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, test_sequences)), dtype=int)))\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Max train sequence length: {}'.format(np.max(list(map(len, sequences)))))\n",
    "print('Max test sequence length: {}'.format(np.max(list(map(len, test_sequences)))))\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Min train sequence length: {}'.format(np.min(list(map(len, sequences)))))\n",
    "print('Min test sequence length: {}'.format(np.min(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 508584 unique tokens\n",
      "Shape of data tensor: (1161066, 300)\n",
      "Shape of label tensor: (1161066, 5)\n",
      "Shape of test_data tensor: (334999, 300)\n"
     ]
    }
   ],
   "source": [
    "# Name word index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "# Format data so that it'll fit into the neural network\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example conversion of text to padded Sequence:\n",
      "Only had it for 2 weeks, but so far it seems functional. While editing photos (pushing the computer hard) the battery lasts 2.5'ish hours.Placed it in a Dv6 1030 laptop.\n",
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0     56     42\n",
      "      6     10     71    579     18     22    161      6    206   1371\n",
      "    128   1577    454   2366      1    126    150      1    109   1520\n",
      "     71 205189    337   1257      6     12      5  10586  18823    143]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example conversion of text to padded Sequence:\")\n",
    "print(comments[0])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Their Approach\n",
    "\n",
    "\n",
    "- Modeling Techniques\t\n",
    "1. Logistic Regression \n",
    "2. Naïve Bayes\n",
    "3. Random Forest\n",
    "4. LDA\n",
    "5. SVM\n",
    "6. XGBoost\n",
    "7. Neural Networks (RNN-LSTM)\n",
    " - Calculate Loss: MSE\n",
    " - Write as multivariable function\n",
    "     - This system of calculating partial derivatives by working backwards is known as backpropagation, or “backprop”.\n",
    " - Train \n",
    "Before we train our network, we first need a way to quantify how “good” it’s doing so that it can try to do “better”. That’s what the loss is.\n",
    "We’ll use the mean squared error (MSE) loss:\n",
    "We now have a clear goal: minimize the loss of the neural network. We know we can change the network’s weights and biases to influence its predictions, but how do we do so in a way that decreases loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Our Approach\n",
    "- NEAT\n",
    "- Comparison to traditional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines the fitness function for a genome as the accuracy of its neural network\n",
    "# Evaluates the fitness by building and testing the evolved neural network\n",
    "def category_match(calculated, correct):\n",
    "    rounded = list(map(round, calculated))\n",
    "    matches = 0\n",
    "    for category in range(0,len(correct)):\n",
    "        if rounded[category] == correct[category]:\n",
    "            matches += 1\n",
    "    return matches/(len(correct)-1)\n",
    "    \n",
    "def eval_genomes(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness = 0 # default fitness (should be very bad)\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        count = 0\n",
    "        matches = 0\n",
    "        for xi, xo in zip(data[:limit], y[:limit]):\n",
    "            output = net.activate(xi)\n",
    "            count += 1\n",
    "            matches += category_match(output, xo)\n",
    "        # The fittest individual has the most correct categorizations\n",
    "        genome.fitness = matches/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Runs the genetic algorithm\n",
    "def run_ga(config_file):\n",
    "    # Load configuration.\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                         config_file)\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = neat.Population(config)\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    p.add_reporter(neat.Checkpointer(5))\n",
    "\n",
    "    # Run for up to 300 generations.\n",
    "    winner = p.run(eval_genomes, max_generations)\n",
    "\n",
    "    # Display the winning genome (Very ugly)\n",
    "#     print('\\nBest genome:\\n{!s}'.format(winner))\n",
    "\n",
    "    # Show output of the most fit genome against training data.\n",
    "    #     @TODO: Replace with validation set here\n",
    "    print('\\nMost fit individual performance:')\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    for xi, xo in zip(data[:limit], y[:limit]):\n",
    "        output = winner_net.activate(xi)\n",
    "        print(\"input {!r}, expected output {!r}, got {!r}\".format(xi, xo, output))\n",
    "\n",
    "#     visualize.draw_net(config, winner, True)\n",
    "#     visualize.plot_stats(stats, ylog=False, view=True)\n",
    "#     visualize.plot_species(stats, view=True)\n",
    "\n",
    "#     p = neat.Checkpointer.restore_checkpoint('neat-checkpoint-4')\n",
    "    p.run(eval_genomes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_generations 100\n",
      "\n",
      " ****** Running generation 0 ****** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"max_generations\",max_generations)\n",
    "run_ga(ga_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [chatbot]",
   "language": "python",
   "name": "Python [chatbot]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
