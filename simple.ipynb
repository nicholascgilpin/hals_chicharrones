{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lessons from the Machine Learning Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- Introduction\n",
    "- AI at Dell (AI COE)\n",
    "- The Hackathon\n",
    "- Algorithms Introduction\n",
    "- Common Challenges\n",
    "    - NLP Pre-processing/Feature Engineering\n",
    "- Their approach\n",
    "- Our approach\n",
    "- Q & A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI at Dell\n",
    "- AI COE\n",
    "- Purpose\n",
    "- Teams\n",
    "- Resources\n",
    "- website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackathon\n",
    "- The Problem\n",
    "- The Resources\n",
    "- Teams and Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Introduction\n",
    "\n",
    "### Neural Networks\n",
    "- Architectures\n",
    "- Back-propation\n",
    "\n",
    "### Genetic Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Challenges\n",
    "- Pre-processing\n",
    "- Feature Engineering\n",
    "- Resource Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "from __future__ import print_function\n",
    "import neat\n",
    "import visualize\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU,CuDNNLSTM,CuDNNGRU, Reshape, MaxPooling1D,AveragePooling1D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Program Parameters\n",
    "TRAIN_DATA_FILE = \"train.csv\"\n",
    "ga_config = \"ga_config.txt\" # Parameters for the genetic algorithm\n",
    "max_features = 250000 # Maximum Number of Words in Dictionary\n",
    "maxlen = 300   # Maximum Sequence Size\n",
    "\n",
    "# Genetic Algorithm Hyperparameters\n",
    "max_generations = 100 # previously 300\n",
    "limit = 100000 # limit number of rows to train per network\n",
    "# Note: limit must match fitness_threshold in ga_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data from train.csv\n",
      ".\n",
      ".\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Load Files\n",
    "print (\"Loading Data from \" + str(TRAIN_DATA_FILE))\n",
    "final_validation_file = pd.read_csv(TRAIN_DATA_FILE)[:176230]  # (176230)\n",
    "print(\".\")\n",
    "test_df = pd.read_csv(TRAIN_DATA_FILE)[176231:511230] # (335000)\n",
    "print(\".\")\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)[511231:1672297]\n",
    "print(\"done!\")\n",
    "total_rows = len(train_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               ReviewText  Rating\n",
      "511231  Only had it for 2 weeks, but so far it seems f...       4\n",
      "511232  Great to deal with this company.  Turned out m...       5\n",
      "511233  It does the job for a very low price what more...       5\n",
      "511234  My brother will be very happy, I gave him my H...       5\n",
      "511235  I do not subject my battery to a lot of wear a...       2\n",
      "Total rows:  1161066\n"
     ]
    }
   ],
   "source": [
    "# Data preview\n",
    "print(train_df.head())\n",
    "print(\"Total rows: \", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare Data \n",
      ".\n",
      ".\n",
      ".\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare Data \")\n",
    "print(\".\")\n",
    "list_sentences_train = train_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "list_classes = [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\n",
    "num_classes=5\n",
    "print(\".\")\n",
    "#y = train_df[list_classes].values\n",
    "target=train_df['Rating'].values\n",
    "y1=to_categorical(target)\n",
    "y=np.delete(y1, 0, axis=1)\n",
    "print(\".\")\n",
    "list_sentences_test = test_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "yaux=y[:,[0]]\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- ngrams\n",
    "- other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training sentences into arrays of words \n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split test sentences into arrays of words \n",
    "\n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create an index and summary of words in word arrays\n",
      "Convert arrays of words into bag-of-word arrays\n"
     ]
    }
   ],
   "source": [
    "print(\"Create an index and summary of words in word arrays\")\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "print(\"Convert arrays of words into bag-of-word arrays\")\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a summary of data\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Average train sequence length: 119\n",
      "Average test sequence length: 118\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Max train sequence length: 6344\n",
      "Max test sequence length: 5626\n",
      "1161066 train sequences\n",
      "334999 test sequences\n",
      "Min train sequence length: 0\n",
      "Min test sequence length: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Print a summary of data\")\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, sequences)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, test_sequences)), dtype=int)))\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Max train sequence length: {}'.format(np.max(list(map(len, sequences)))))\n",
    "print('Max test sequence length: {}'.format(np.max(list(map(len, test_sequences)))))\n",
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Min train sequence length: {}'.format(np.min(list(map(len, sequences)))))\n",
    "print('Min test sequence length: {}'.format(np.min(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 508584 unique tokens\n",
      "Shape of data tensor: (1161066, 300)\n",
      "Shape of label tensor: (1161066, 5)\n",
      "Shape of test_data tensor: (334999, 300)\n"
     ]
    }
   ],
   "source": [
    "# Name word index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "# Format data so that it'll fit into the neural network\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example conversion of text to padded Sequence:\n",
      "Only had it for 2 weeks, but so far it seems functional. While editing photos (pushing the computer hard) the battery lasts 2.5'ish hours.Placed it in a Dv6 1030 laptop.\n",
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0     56     42\n",
      "      6     10     71    579     18     22    161      6    206   1371\n",
      "    128   1577    454   2366      1    126    150      1    109   1520\n",
      "     71 205189    337   1257      6     12      5  10586  18823    143]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example conversion of text to padded Sequence:\")\n",
    "print(comments[0])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-627d82b7f351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Their Approach\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m-\u001b[0m \u001b[0mModels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Models' is not defined"
     ]
    }
   ],
   "source": [
    "## Their Approach\n",
    "- Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Our Approach\n",
    "- NEAT\n",
    "- Comparison to traditional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines the fitness function for a genome as the accuracy of its neural network\n",
    "# Evaluates the fitness by building and testing the evolved neural network\n",
    "def category_match(calculated, correct):\n",
    "    rounded = list(map(round, calculated))\n",
    "    matches = 0\n",
    "    for category in range(0,len(correct)):\n",
    "        if rounded[category] == correct[category]:\n",
    "            matches += 1\n",
    "    return matches/(len(correct)-1)\n",
    "    \n",
    "def eval_genomes(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness = 0 # default fitness (should be very bad)\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        count = 0\n",
    "        matches = 0\n",
    "        for xi, xo in zip(data[:limit], y[:limit]):\n",
    "            output = net.activate(xi)\n",
    "            count += 1\n",
    "            matches += category_match(output, xo)\n",
    "        # The fittest individual has the most correct categorizations\n",
    "        genome.fitness = matches/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Runs the genetic algorithm\n",
    "def run_ga(config_file):\n",
    "    # Load configuration.\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                         config_file)\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = neat.Population(config)\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    p.add_reporter(neat.Checkpointer(5))\n",
    "\n",
    "    # Run for up to 300 generations.\n",
    "    winner = p.run(eval_genomes, max_generations)\n",
    "\n",
    "    # Display the winning genome (Very ugly)\n",
    "#     print('\\nBest genome:\\n{!s}'.format(winner))\n",
    "\n",
    "    # Show output of the most fit genome against training data.\n",
    "    #     @TODO: Replace with validation set here\n",
    "    print('\\nMost fit individual performance:')\n",
    "    winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "    for xi, xo in zip(data[:limit], y[:limit]):\n",
    "        output = winner_net.activate(xi)\n",
    "        print(\"input {!r}, expected output {!r}, got {!r}\".format(xi, xo, output))\n",
    "\n",
    "#     visualize.draw_net(config, winner, True)\n",
    "#     visualize.plot_stats(stats, ylog=False, view=True)\n",
    "#     visualize.plot_species(stats, view=True)\n",
    "\n",
    "#     p = neat.Checkpointer.restore_checkpoint('neat-checkpoint-4')\n",
    "    p.run(eval_genomes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_generations 100\n",
      "\n",
      " ****** Running generation 0 ****** \n",
      "\n",
      "Population's average fitness: 0.62202 stdev: 0.03898\n",
      "Best fitness: 0.70096 - size: (5, 1500) - species 1 - id 30\n",
      "Average adjusted fitness: 0.073\n",
      "Mean genetic distance 1.203, standard deviation 0.173\n",
      "Population of 50 members in 1 species:\n",
      "   ID   age  size  fitness  adj fit  stag\n",
      "  ====  ===  ====  =======  =======  ====\n",
      "     1    0    50      0.7    0.073     0\n",
      "Total extinctions: 0\n",
      "Generation time: 13666.573 sec\n",
      "Saving checkpoint to neat-checkpoint-0\n",
      "\n",
      " ****** Running generation 1 ****** \n",
      "\n",
      "Population's average fitness: 0.66702 stdev: 0.02984\n",
      "Best fitness: 0.73495 - size: (5, 1495) - species 1 - id 62\n",
      "Average adjusted fitness: 0.100\n",
      "Mean genetic distance 1.264, standard deviation 0.190\n",
      "Population of 50 members in 1 species:\n",
      "   ID   age  size  fitness  adj fit  stag\n",
      "  ====  ===  ====  =======  =======  ====\n",
      "     1    1    50      0.7    0.100     0\n",
      "Total extinctions: 0\n",
      "Generation time: 13696.097 sec (13681.335 average)\n",
      "Saving checkpoint to neat-checkpoint-1\n",
      "\n",
      " ****** Running generation 2 ****** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"max_generations\",max_generations)\n",
    "run_ga(ga_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [chatbot]",
   "language": "python",
   "name": "Python [chatbot]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
