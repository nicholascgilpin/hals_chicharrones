{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU,CuDNNLSTM,CuDNNGRU, Reshape, MaxPooling1D,AveragePooling1D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somthing\n",
      "<bound method DataFrame.count of                                                 ReviewText  Rating\n",
      "511231   Only had it for 2 weeks, but so far it seems f...       4\n",
      "511232   Great to deal with this company.  Turned out m...       5\n",
      "511233   It does the job for a very low price what more...       5\n",
      "511234   My brother will be very happy, I gave him my H...       5\n",
      "511235   I do not subject my battery to a lot of wear a...       2\n",
      "511236   A great replacement battery. It has a fantasti...       5\n",
      "511237   I admit I have not done a full test and such. ...       5\n",
      "511238   If you just need a battery for short periods o...       3\n",
      "511239   Used for an HP 1230us. Battery fit is right. G...       4\n",
      "511240   This battery isn't like the cheap batteries yo...       5\n",
      "511241   I bough this for my computer when its old batt...       5\n",
      "511242   From day one it behaved strange not fully char...       3\n",
      "511243   This battery is a fraction of the price of the...       5\n",
      "511244   Great service and fast at a good price... Batt...       5\n",
      "511245   DO NOT Buy This Battery For HP Pavilion DV6The...       1\n",
      "511246   The battery does not fit very well.  It keeps ...       1\n",
      "511247   This is a great product and worked for me!  I ...       5\n",
      "511248   I bough this and wasn't quite sure I had made ...       5\n",
      "511249   Great product. I received it in the mail way b...       5\n",
      "511250   This is customer service the way it should be....       4\n",
      "511251   buying 3rd party laptop batteries is always a ...       3\n",
      "511252   A nice replacement for a laptop battery. Batte...       5\n",
      "511253   Needed to swap out a battery that went dead an...       5\n",
      "511254   With my T60p I am only seeing one hour of use ...       2\n",
      "511255   After reading reviews and the price I was look...       2\n",
      "511256   This battery works quite well in my IBM T60 la...       5\n",
      "511257   What a complete waste of money. First off, you...       1\n",
      "511258   Meets exact specifications of the original bat...       5\n",
      "511259   I'd purchased itNEW Laptop/Notebook Battery fo...       5\n",
      "511260   I have two laptops. An R60 and an R61, both of...       1\n",
      "...                                                    ...     ...\n",
      "1672267  Great design, no loss of sound, small enough t...       5\n",
      "1672268  I received this at no cost in exchange for wri...       3\n",
      "1672269  Want to add wireless audio streaming to your h...       5\n",
      "1672270  I've never seen something quite like this befo...       4\n",
      "1672271  ...all part of the package that comes with a p...       5\n",
      "1672272  The Blaster's latest was sent to me as a 'for ...       5\n",
      "1672273  Imagine having a speaker that can be connected...       5\n",
      "1672274  Received this this week and couldn't be happie...       5\n",
      "1672275  Wow...that's about all I have to say.  Having ...       5\n",
      "1672276  I've been a long time Creative customer since ...       5\n",
      "1672277  Update 14 July 2014: they have a white paper a...       5\n",
      "1672278  I've had my hands on a lot of speakers over th...       5\n",
      "1672279  I'm a \"get it out of the box and just plug and...       5\n",
      "1672280  The Creative Sound Blaster Roar puts out impre...       5\n",
      "1672281  The Sound Blaster Roar is one of the most perf...       5\n",
      "1672282  The names Creative Labs and Sound Blaster are ...       5\n",
      "1672283  I am not an acoustical expert nor a musician o...       5\n",
      "1672284  Very impressive. Loud, clear and audiophile qu...       5\n",
      "1672285  [Please read this review for full details and ...       5\n",
      "1672286  Bluetooth speakers have improved a lot over th...       5\n",
      "1672287  Super sonic speaker system! Can you say that 5...       5\n",
      "1672288  Disclosure: I received a free sample of this i...       5\n",
      "1672289  My short review:If you have the money to spend...       5\n",
      "1672290  The Creative Sound Blaster Roar has a powerful...       5\n",
      "1672291  Move over Bose, JBL and Klipsch! There's a new...       5\n",
      "1672292  Burned these in before listening to them for a...       5\n",
      "1672293  Some people like DJ style headphones or earbud...       5\n",
      "1672294  I&#8217;m a big fan of the Brainwavz S1 (actua...       5\n",
      "1672295  I've used theBrainwavz S1 In Ear Headphones, a...       5\n",
      "1672296  Normally when I receive a review sample I can ...       5\n",
      "\n",
      "[1161066 rows x 2 columns]>\n",
      "<bound method DataFrame.count of                                                ReviewText  Rating\n",
      "176231  These Portable Speakers are a surprisingly goo...       4\n",
      "176232  These speakers sound great considering their s...       5\n",
      "176233  Beware all audophiles: the sound from this pro...       4\n",
      "176234  These speakers are super convenient. They fold...       5\n",
      "176235  This are very good little speakers you can tak...       4\n",
      "176236  As always, one has to have a realistic expecta...       5\n",
      "176237  The problem with these speakers is the glue us...       1\n",
      "176238  I bought these speakers for $4.36 to use at th...       4\n",
      "176239  i got these speakers for $1.30 plus $3 shippin...       4\n",
      "176240  ...they're fine. I teach, and this is great to...       4\n",
      "176241  I purchased four different ear phones, includi...       5\n",
      "176242  The sound becomes slightly garbled at the high...       4\n",
      "176243  There is a USB power cord that I pluged into t...       4\n",
      "176244  It's under $10, what do you expect!?!  This mo...       3\n",
      "176245  I received these speakers as a present, then f...       4\n",
      "176246  The first set I received did not work, but I w...       3\n",
      "176247  Really decent sound for the size and the cost....       4\n",
      "176248  Purchased these little guys for my wifes opera...       4\n",
      "176249  I purchased these speakers to take with me on ...       2\n",
      "176250  I was looking for some cheap low volume speake...       4\n",
      "176251  These speakers need to be connected to a USB p...       3\n",
      "176252  The speakers are nice little units that are go...       3\n",
      "176253  I bought these speakers because they were chea...       2\n",
      "176254  Bought this to have music in the bathroom whil...       2\n",
      "176255  These speakers are amazing for the price. Othe...       5\n",
      "176256  I give five stars based on the performance and...       5\n",
      "176257  These speakers blew up the first time I used t...       1\n",
      "176258  I'm happy. Good for the money use it at work a...       4\n",
      "176259  For some reason, the sound card on my Midnight...       4\n",
      "176260  I am always looking for speakers to go on my I...       4\n",
      "...                                                   ...     ...\n",
      "511200  Lasts barely an hour on my Pavilion dv6 with t...       3\n",
      "511201  Received the battery on time. It holds a charg...       4\n",
      "511202  Easy to pop in, fits good, and even came half ...       5\n",
      "511203  I have only used it for a few days but it seem...       5\n",
      "511204  I was so happy to get this battery.  I has bee...       5\n",
      "511205  fit in my notebook and it works just fine. Loa...       5\n",
      "511206  Great product so far it seems to keep a charge...       5\n",
      "511207  I bought this battery for my hp G60 series lap...       5\n",
      "511208  Working fine.LIfe after charge is as expected ...       5\n",
      "511209  I purchased this battery as a replacement for ...       4\n",
      "511210  We bought this as a replacement battery for ou...       5\n",
      "511211  It fit in like a glove and works the way it is...       4\n",
      "511212  Straight out of the box I charged it and used ...       5\n",
      "511213  HThis battery is good no problems at all. Work...       5\n",
      "511214  I really like htis Battery and the lifetime is...       4\n",
      "511215  I bought this to replace the factory battery, ...       1\n",
      "511216  Well...I ordered the battery in brand new cond...       1\n",
      "511217  Battery was for an older Compaq that the origi...       3\n",
      "511218  Realized I needed a new battery for the laptop...       4\n",
      "511219  I was pretty worried when I was seeing all of ...       5\n",
      "511220  This is a good replacement for acer extensa ba...       5\n",
      "511221  This battery fits my Acer Extensa 4620Z perfec...       5\n",
      "511222  i don't have a problem with the seller, my pro...       2\n",
      "511223  When I purchased it everything on the descript...       2\n",
      "511224  I can't lie, this battery is incredibly cheap ...       5\n",
      "511225  Battery was a perfect fit, No error using the ...       5\n",
      "511226  I bought this battery, but my computer started...       1\n",
      "511227  I have problems with the battery, but I'm not ...       2\n",
      "511228  I needed to replace the battery in my daughter...       5\n",
      "511229  I bought this to replace a near-dead battery o...       1\n",
      "\n",
      "[334999 rows x 2 columns]>\n",
      "<bound method DataFrame.count of                                                ReviewText  Rating\n",
      "0       We got this GPS for my husband who is an (OTR)...       5\n",
      "1       I'm a professional OTR truck driver, and I bou...       1\n",
      "2       Well, what can I say.  I've had this unit in m...       3\n",
      "3       Not going to write a long review, even thought...       2\n",
      "4       I've had mine for a year and here's what we go...       1\n",
      "5       I am using this with a Nook HD+. It works as d...       5\n",
      "6       The cable is very wobbly and sometimes disconn...       2\n",
      "7       This adaptor is real easy to setup and use rig...       5\n",
      "8       This adapter easily connects my Nook HD 7&#34;...       4\n",
      "9       This product really works great but I found th...       5\n",
      "10      This item is just as was described in the orig...       4\n",
      "11      bought for a spare for my 9&#34; Nook HD and i...       5\n",
      "12      My son crewed my HD charger cord so I needed a...       5\n",
      "13      This is a good beefy 2 amp charger, but it cov...       3\n",
      "14      I lost my B&N original cable.  I looked around...       5\n",
      "15      It does 2A and charges a DEAD Nook in a few ho...       3\n",
      "16      Go to Target or Barnes and Noble instead, and ...       3\n",
      "17      Works well, a little pricey I think for a char...       4\n",
      "18      This is a great buy, compared to a $60 or more...       5\n",
      "19      This mount is just what I needed.  It is stron...       5\n",
      "20      Great deal, easy to mount and it appears to be...       5\n",
      "21      This mount works really well once you get it u...       4\n",
      "22      This wall mount does everything it's supposed ...       4\n",
      "23      for the price you just cant beat this item. I ...       5\n",
      "24      I received the mount, which was well packaged ...       4\n",
      "25      Took a bit of work, but used this to fit an ol...       4\n",
      "26      I used this for my 47&#34; Samsung. Its fit/fe...       5\n",
      "27      This item serves its purpose as well as any ot...       5\n",
      "28      I love this mount.  This is the 4th one I've b...       5\n",
      "29      I am very glad for buying it.  It is heavy dut...       5\n",
      "...                                                   ...     ...\n",
      "176200  Works as described . I recommend buying at lea...       4\n",
      "176201  Does exactly what I need it to. No problems at...       5\n",
      "176202  Bought it to connect connect to shorter cables...       5\n",
      "176203  This usb is a simple one with the knowledge of...       4\n",
      "176204  This little item works as advertised and what ...       5\n",
      "176205  This tiny connector was just the thing for me:...       5\n",
      "176206  I needed a female to female USB adapter and wa...       5\n",
      "176207  Arrived VERY quick. What can I say.. it's a fa...       5\n",
      "176208  I bought two of these fan guards to use as bak...       5\n",
      "176209  What more can one say about a fan guard.  It w...       5\n",
      "176210  i use to laugh at these but if you do much mod...       5\n",
      "176211  Has a slight concave shape such that the grill...       5\n",
      "176212  these fan guards work and fit the fans they ar...       3\n",
      "176213  The grills worked great in my server. They do ...       5\n",
      "176214  Straight forward Fan Guard.  Looks and perform...       5\n",
      "176215  Well made and perfect fit on my Corsair fans. ...       5\n",
      "176216  Keeps the cables out of my fans but didn't com...       4\n",
      "176217  Received this fan grill. I installed this in m...       5\n",
      "176218  Does what it supposed to.  Came in nice zip lo...       5\n",
      "176219  It is easy to install, looks nice, and does ex...       5\n",
      "176220  So much easier to open, you can just replace a...       5\n",
      "176221  When I drop a case screw, it inevitably rolls ...       4\n",
      "176222  Computer cases for desktop computers often com...       4\n",
      "176223  The use of these brass thumbscrews is totally ...       5\n",
      "176224  Going to get more and use them in place of scr...       5\n",
      "176225  Good sound and easy to use. Does not come with...       5\n",
      "176226  great little gadget. excellent price cannot be...       5\n",
      "176227  I bought this device to listen to my Sansa whi...       4\n",
      "176228  When testing the product out the on button wou...       5\n",
      "176229  I was hoping to get a small set of speakers th...       4\n",
      "\n",
      "[176230 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "maxlen = 300   # Maximum Sequence Size \n",
    "max_features = 250000 # Maximum Number of Words in Dictionary\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300 #300\n",
    "num_dense = 256 # 256\n",
    "rate_drop_lstm = 0.05\n",
    "rate_drop_dense = rate_drop_lstm\n",
    "\n",
    "loss=\"val_acc\"\n",
    "#loss=\"val_loss\"\n",
    "opt='rmsprop'\n",
    "#opt='adam'\n",
    "\n",
    "lr=0.01\n",
    "from keras.optimizers import RMSprop, SGD, Nadam, Adamax, Adam\n",
    "#opto = SGD(lr=lr, clipvalue=0.5)\n",
    "#opto=  Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#opto = RMSprop (lr=lr)\n",
    "#opto = Nadam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "#opto = Adamax(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "##\n",
    " \n",
    "act = 'relu'\n",
    "Trainable=True\n",
    "\n",
    "\n",
    "data_folder = Path(\"C:/Users/joshua_giles/Downloads/train\")\n",
    "\n",
    "TRAIN_DATA_FILE = data_folder / \"train.csv\"\n",
    "\n",
    "print (\"somthing\")\n",
    "\n",
    "secret_saucy_df = pd.read_csv(TRAIN_DATA_FILE)[:176230]  # (176230)\n",
    "validate_df = pd.read_csv(TRAIN_DATA_FILE)[176231:511230] # (335000)\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)[511231:1672297]\n",
    "\n",
    "print(train_df.count)\n",
    "print(validate_df.count)\n",
    "print(secret_saucy_df.count)\n",
    "\n",
    "#print(list_sentences_train)\n",
    "#f = open(file_to_open)\n",
    "\n",
    "#print(list(train_df))\n",
    "list_sentences_train = train_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "list_classes = [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\n",
    "num_classes=5\n",
    "#y = train_df[list_classes].values\n",
    "target=train_df['Rating'].values\n",
    "y1=to_categorical(target)\n",
    "y=np.delete(y1, 0, axis=1)\n",
    "list_sentences_test = validate_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "yaux=y[:,[0]]\n",
    "#print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitstring(s):\n",
    "    # searching the number of characters to split on\n",
    "    proposed_pattern = s[0]\n",
    "    for i, c in enumerate(s[1:], 1):\n",
    "        if c != \" \":\n",
    "            if proposed_pattern == s[i:(i+len(proposed_pattern))]:\n",
    "                # found it\n",
    "                break\n",
    "            else:\n",
    "                proposed_pattern += c\n",
    "    else:\n",
    "        exit(1)\n",
    "\n",
    "    return proposed_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(s):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text,to_lower=False, rem_urls=False, rem_3plus=False, \\\n",
    "                     split_repeated=False, rem_special=False, rep_num=False,\n",
    "                     man_adj=True, rem_stopwords=False, stem_snowball=False,\\\n",
    "                     stem_porter=False, lemmatize=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    if rem_urls:\n",
    "        text = remove_urls(text)\n",
    "    if to_lower:    \n",
    "        text = text.lower()\n",
    "    if rem_3plus:    \n",
    "        text = ReplaceThreeOrMore(text)\n",
    "\n",
    "    if man_adj: \n",
    "        # Clean the text\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" ! \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "        text = re.sub(r\"\\+\", \" + \", text)\n",
    "        text = re.sub(r\"\\-\", \" - \", text)\n",
    "        text = re.sub(r\"\\=\", \" = \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = re.sub(r\":\", \" : \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        text = re.sub(r\"\\0s\", \"0\", text)\n",
    "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "        text = re.sub(r\"e - mail\", \"email\", text)\n",
    "        text = re.sub(r\"j k\", \"jk\", text)\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # split them into a list\n",
    "    text = text.split()\n",
    "    \n",
    "    if split_repeated:\n",
    "        for i, c in enumerate(text):\n",
    "            text[i]=splitstring(c)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if rem_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    if rem_special: \n",
    "        text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    if rep_num:     \n",
    "        text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_snowball:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    if stem_porter:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "        \n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])   \n",
    " \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 train sequences\n",
      "334999 test sequences\n",
      "Average train sequence length: 106\n",
      "Average test sequence length: 120\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, sequences)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, test_sequences)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 train sequences\n",
      "334999 test sequences\n",
      "Max train sequence length: 865\n",
      "Max test sequence length: 5719\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Max train sequence length: {}'.format(np.max(list(map(len, sequences)))))\n",
    "print('Max test sequence length: {}'.format(np.max(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 train sequences\n",
      "334999 test sequences\n",
      "Min train sequence length: 14\n",
      "Min test sequence length: 1\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Min train sequence length: {}'.format(np.min(list(map(len, sequences)))))\n",
    "print('Min test sequence length: {}'.format(np.min(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184130 unique tokens\n",
      "Shape of data tensor: (50, 300)\n",
      "Shape of label tensor: (1161066, 5)\n",
      "Shape of test_data tensor: (334999, 300)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del comments\n",
    "del test_sequences\n",
    "del sequences\n",
    "del list_sentences_train\n",
    "del list_sentences_test\n",
    "del train_df\n",
    "#del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing GLOVE-840B vectors\n",
      "Vector C:\\Users\\joshua_giles\\Downloads\\train\\glove.840B.300d.txt\n",
      "Total 94 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index GLOVE  word vectors\n",
    "########################################\n",
    "\n",
    "EMBEDDING_FILE=data_folder  / \"glove.840B.300d.txt\"\n",
    "et=\"GLOVE-840B\"\n",
    "\n",
    "#EMBEDDING_FILE=path+'glove/glove.6B.300d.txt'\n",
    "#et=\"GLOVE-6B\"\n",
    "\n",
    "#EMBEDDING_FILE= path+'prodata/toxic_clean_300d.txt'\n",
    "#et='TOXIC-TXT'\n",
    "\n",
    "#EMBEDDING_FILE= path+'fasttext/crawl-300d-2M.vec'\n",
    "#et=\"FASTTEXT\"\n",
    "\n",
    "\n",
    "print('Indexing '+et+' vectors')\n",
    "print(\"Vector\",EMBEDDING_FILE )\n",
    "#Glove Vectors\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE,  encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    #word = values[0]\n",
    "    #coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua_giles\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "## Glove\n",
    "#########\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std\n",
    "\n",
    "########################################\n",
    "## GLOVE prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(max_features, len(word_index))+1\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\n",
    "#embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "nl=0\n",
    "gd=0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        #print ('Over: ',word)\n",
    "        nl = nl +1 \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        gd=gd+1\n",
    "    else:\n",
    "        #print (word)\n",
    "        nl = nl +1 \n",
    "\n",
    "         \n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix (184131, 300)\n",
      "Tamanho Vocabulario 184130 Maximo de Features 250000\n",
      "Null word embeddings: 184094\n",
      "Good word embeddings: 36\n"
     ]
    }
   ],
   "source": [
    "#print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0)) \n",
    "print( \"Matrix\", embedding_matrix.shape)\n",
    "print( \"Tamanho Vocabulario\", len(word_index), \"Maximo de Features\",max_features)\n",
    "print('Null word embeddings: %d' % nl)\n",
    "print('Good word embeddings: %d' % gd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_model0():\n",
    "    \n",
    "    mdln=\"00-max-d-dr-d-dr-d-dr\"\n",
    "    comment_input = Input((maxlen,))\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    comment_emb = Embedding(max_features, EMBEDDING_DIM, input_length=maxlen, \n",
    "                            embeddings_initializer=\"uniform\")(comment_input)\n",
    "\n",
    "    # we add a GlobalMaxPooling1D, which will extract features from the embeddings\n",
    "    # of all words in the comment\n",
    "    m = GlobalMaxPooling1D()(comment_emb)\n",
    "    d = Dense(1024, activation=act)(m)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(512, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(256, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(128, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    # We project onto a six-unit output layer, and squash it with a sigmoid:\n",
    "    output = Dense(num_classes, activation='sigmoid')(d)\n",
    "\n",
    "    model = Model(inputs=comment_input, outputs=output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    #for layer in model.layers:\n",
    "     #   weights = layer.get_weights()\n",
    "    #print(weights)\n",
    "    \n",
    "    return model,mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Modelo LSTM Base Line \n",
    "########################################\n",
    "def create_model1():\n",
    "\n",
    "    mdln=\"01-lstm-att-d-b\"\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=Trainable)\n",
    "\n",
    "    lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n",
    "\n",
    "    comment_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences= embedding_layer(comment_input)\n",
    "    x = lstm_layer(embedded_sequences)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    merged = Attention(maxlen)(x)\n",
    "    merged = Dense(num_dense, activation=act)(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    preds = Dense(num_classes, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[comment_input], \\\n",
    "            outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "   \n",
    "    return model, mdln"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
