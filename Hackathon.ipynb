{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-04884d1ed301>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU,CuDNNLSTM,CuDNNGRU, Reshape, MaxPooling1D,AveragePooling1D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "maxlen = 300   # Maximum Sequence Size \n",
    "max_features = 250000 # Maximum Number of Words in Dictionary\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300 #300\n",
    "num_dense = 256 # 256\n",
    "rate_drop_lstm = 0.05\n",
    "rate_drop_dense = rate_drop_lstm\n",
    "\n",
    "loss=\"val_acc\"\n",
    "#loss=\"val_loss\"\n",
    "opt='rmsprop'\n",
    "#opt='adam'\n",
    "\n",
    "lr=0.01\n",
    "from keras.optimizers import RMSprop, SGD, Nadam, Adamax, Adam\n",
    "#opto = SGD(lr=lr, clipvalue=0.5)\n",
    "#opto=  Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#opto = RMSprop (lr=lr)\n",
    "#opto = Nadam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "#opto = Adamax(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "##\n",
    " \n",
    "act = 'relu'\n",
    "Trainable=True\n",
    "\n",
    "\n",
    "data_folder = Path(\"C:/Users/joshua_giles/Downloads/train\")\n",
    "\n",
    "TRAIN_DATA_FILE = data_folder / \"train.csv\"\n",
    "\n",
    "print (\"somthing\")\n",
    "\n",
    "secret_saucy_df = pd.read_csv(TRAIN_DATA_FILE)[:176230]  # (176230)\n",
    "validate_df = pd.read_csv(TRAIN_DATA_FILE)[176231:511230] # (335000)\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)[511231:1672297]\n",
    "\n",
    "print(train_df.count)\n",
    "print(validate_df.count)\n",
    "print(secret_saucy_df.count)\n",
    "\n",
    "#print(list_sentences_train)\n",
    "#f = open(file_to_open)\n",
    "\n",
    "#print(list(train_df))\n",
    "list_sentences_train = train_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "list_classes = [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\n",
    "num_classes=5\n",
    "#y = train_df[list_classes].values\n",
    "target=train_df['Rating'].values\n",
    "y1=to_categorical(target)\n",
    "y=np.delete(y1, 0, axis=1)\n",
    "list_sentences_test = validate_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "yaux=y[:,[0]]\n",
    "#print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitstring(s):\n",
    "    # searching the number of characters to split on\n",
    "    proposed_pattern = s[0]\n",
    "    for i, c in enumerate(s[1:], 1):\n",
    "        if c != \" \":\n",
    "            if proposed_pattern == s[i:(i+len(proposed_pattern))]:\n",
    "                # found it\n",
    "                break\n",
    "            else:\n",
    "                proposed_pattern += c\n",
    "    else:\n",
    "        exit(1)\n",
    "\n",
    "    return proposed_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(s):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text,to_lower=False, rem_urls=False, rem_3plus=False, \\\n",
    "                     split_repeated=False, rem_special=False, rep_num=False,\n",
    "                     man_adj=True, rem_stopwords=False, stem_snowball=False,\\\n",
    "                     stem_porter=False, lemmatize=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    if rem_urls:\n",
    "        text = remove_urls(text)\n",
    "    if to_lower:    \n",
    "        text = text.lower()\n",
    "    if rem_3plus:    \n",
    "        text = ReplaceThreeOrMore(text)\n",
    "\n",
    "    if man_adj: \n",
    "        # Clean the text\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" ! \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "        text = re.sub(r\"\\+\", \" + \", text)\n",
    "        text = re.sub(r\"\\-\", \" - \", text)\n",
    "        text = re.sub(r\"\\=\", \" = \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = re.sub(r\":\", \" : \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        text = re.sub(r\"\\0s\", \"0\", text)\n",
    "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "        text = re.sub(r\"e - mail\", \"email\", text)\n",
    "        text = re.sub(r\"j k\", \"jk\", text)\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # split them into a list\n",
    "    text = text.split()\n",
    "    \n",
    "    if split_repeated:\n",
    "        for i, c in enumerate(text):\n",
    "            text[i]=splitstring(c)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if rem_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    if rem_special: \n",
    "        text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    if rep_num:     \n",
    "        text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_snowball:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    if stem_porter:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "        \n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])   \n",
    " \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, sequences)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, test_sequences)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Max train sequence length: {}'.format(np.max(list(map(len, sequences)))))\n",
    "print('Max test sequence length: {}'.format(np.max(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Min train sequence length: {}'.format(np.min(list(map(len, sequences)))))\n",
    "print('Min test sequence length: {}'.format(np.min(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del comments\n",
    "del test_sequences\n",
    "del sequences\n",
    "del list_sentences_train\n",
    "del list_sentences_test\n",
    "del train_df\n",
    "#del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## index GLOVE  word vectors\n",
    "########################################\n",
    "\n",
    "EMBEDDING_FILE=data_folder  / \"glove.840B.300d.txt\"\n",
    "et=\"GLOVE-840B\"\n",
    "\n",
    "#EMBEDDING_FILE=path+'glove/glove.6B.300d.txt'\n",
    "#et=\"GLOVE-6B\"\n",
    "\n",
    "#EMBEDDING_FILE= path+'prodata/toxic_clean_300d.txt'\n",
    "#et='TOXIC-TXT'\n",
    "\n",
    "#EMBEDDING_FILE= path+'fasttext/crawl-300d-2M.vec'\n",
    "#et=\"FASTTEXT\"\n",
    "\n",
    "\n",
    "print('Indexing '+et+' vectors')\n",
    "print(\"Vector\",EMBEDDING_FILE )\n",
    "#Glove Vectors\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE,  encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    #word = values[0]\n",
    "    #coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "## Glove\n",
    "#########\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std\n",
    "\n",
    "########################################\n",
    "## GLOVE prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(max_features, len(word_index))+1\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\n",
    "#embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "nl=0\n",
    "gd=0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        #print ('Over: ',word)\n",
    "        nl = nl +1 \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        gd=gd+1\n",
    "    else:\n",
    "        #print (word)\n",
    "        nl = nl +1 \n",
    "\n",
    "         \n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0)) \n",
    "print( \"Matrix\", embedding_matrix.shape)\n",
    "print( \"Tamanho Vocabulario\", len(word_index), \"Maximo de Features\",max_features)\n",
    "print('Null word embeddings: %d' % nl)\n",
    "print('Good word embeddings: %d' % gd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_model0():\n",
    "    \n",
    "    mdln=\"00-max-d-dr-d-dr-d-dr\"\n",
    "    comment_input = Input((maxlen,))\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    comment_emb = Embedding(max_features, EMBEDDING_DIM, input_length=maxlen, \n",
    "                            embeddings_initializer=\"uniform\")(comment_input)\n",
    "\n",
    "    # we add a GlobalMaxPooling1D, which will extract features from the embeddings\n",
    "    # of all words in the comment\n",
    "    m = GlobalMaxPooling1D()(comment_emb)\n",
    "    d = Dense(1024, activation=act)(m)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(512, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(256, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(128, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    # We project onto a six-unit output layer, and squash it with a sigmoid:\n",
    "    output = Dense(num_classes, activation='sigmoid')(d)\n",
    "\n",
    "    model = Model(inputs=comment_input, outputs=output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    #for layer in model.layers:\n",
    "     #   weights = layer.get_weights()\n",
    "    #print(weights)\n",
    "    \n",
    "    return model,mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Modelo LSTM Base Line \n",
    "########################################\n",
    "def create_model1():\n",
    "\n",
    "    mdln=\"01-lstm-att-d-b\"\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=Trainable)\n",
    "\n",
    "    lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n",
    "\n",
    "    comment_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences= embedding_layer(comment_input)\n",
    "    x = lstm_layer(embedded_sequences)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    merged = Attention(maxlen)(x)\n",
    "    merged = Dense(num_dense, activation=act)(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    preds = Dense(num_classes, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[comment_input], \\\n",
    "            outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "   \n",
    "    return model, mdln"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
