{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "92c3f76ef6a7463ed19576361755eccb98c59db4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "d9ffdac82ab8a043fd7974284004b6b5f88c0b08"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "3f8acfc335e80016070acf91777b36cc825c6f32"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "#from util.utils import timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7aeba7854195f9f66e420b5499d3d618e542efaa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lib', 'input', 'working', 'src']\n",
      "['customerreviews', 'glove840b300dtxt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"../\"))\n",
    "print(os.listdir(\"../input/\"))\n",
    "debug=False\n",
    "\n",
    "os.mkdir(\"../testpred\")\n",
    "os.mkdir(\"../testpred/sub\")\n",
    "os.mkdir(\"../trainpred\")\n",
    "os.mkdir(\"../trainpred/sub\")\n",
    "os.mkdir(\"../model\")\n",
    "os.mkdir(\"../model/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2cdc2bf81300717d1db01fd18bce272d104330c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU,CuDNNLSTM,CuDNNGRU, Reshape, MaxPooling1D,AveragePooling1D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import colorama\n",
    "from colorama import Fore\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "80ff61c4aac2ecbc366d1d3be18d29609ce14228"
   },
   "outputs": [],
   "source": [
    "path = '../'\n",
    "TRAIN_DATA_FILE=path+'input/customerreviews/train.csv'\n",
    "#TEST_DATA_FILE=path+'input/customerreviews/test1_generic_reviews.csv'\n",
    "#SUB_DATA_FILE=path+'Test1_generic_reviews.csv'\n",
    "TEST_DATA_FILE=path+'input/customerreviews/test2_dell_reviews.csv'\n",
    "SUB_DATA_FILE=path+'Test2_dell_submission.csv'\n",
    "maxlen = 300   # Maximum Sequence Size \n",
    "max_features = 250000 # Maximum Number of Words in Dictionary\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300 #300\n",
    "num_dense = 256 # 256\n",
    "rate_drop_lstm = 0.05\n",
    "rate_drop_dense = rate_drop_lstm\n",
    "\n",
    "loss=\"val_acc\"\n",
    "#loss=\"val_loss\"\n",
    "opt='rmsprop'\n",
    "#opt='adam'\n",
    "\n",
    "lr=0.01\n",
    "from keras.optimizers import RMSprop, SGD, Nadam, Adamax, Adam\n",
    "#opto = SGD(lr=lr, clipvalue=0.5)\n",
    "#opto=  Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#opto = RMSprop (lr=lr)\n",
    "#opto = Nadam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "#opto = Adamax(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "##\n",
    " \n",
    "act = 'relu'\n",
    "Trainable=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "5a141c3ac1771acec19edaf6b09921e2f687a9c2"
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    start_time = datetime.now()\n",
    "    print(f'[{name}] Started : '+start_time.strftime(\"%d-%m-%Y %H:%M\"))\n",
    "    yield\n",
    "    thour, temp_sec = divmod( (datetime.now() - start_time).total_seconds(), 3600)\n",
    "    tmin, tsec = divmod(temp_sec, 60)\n",
    "    print(f'[{name}] Done in :', end=\"\");\n",
    "    print(' %i h %i m and %s seconds.' % (thour, tmin, round(tsec, 2)), end=\"\");\n",
    "    print(f' Ended : '+datetime.now().strftime(\"%d-%m-%Y %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a8c7e0395d25fa2b4c58481a391c8c77a4af9d2e"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b5893043a51f60932aa27e49e8af7cce54997ace"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    # else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    plt.draw()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "63dbf4b345f1e75f8f45d043aac8b3b892a9425a"
   },
   "outputs": [],
   "source": [
    "def multi_roc_auc_score(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    scores = []\n",
    "    for i in range(0, columns):\n",
    "        scr=roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        print (\"Class:\",list_classes[i],\" -roc:{:.5f}\".format(scr))\n",
    "        scores.append(scr)\n",
    "    return np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "f9f9fb6d0388398a261dc1ed885b7eb678ef26ad"
   },
   "outputs": [],
   "source": [
    "def multi_confusion(y_true, y_pred):\n",
    "\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    scores = []\n",
    "    for i in range(0, columns):\n",
    "        rounded_predictions = np.round(y_pred[:, i], 0) \n",
    "        scr=roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        cm = confusion_matrix(y_true[:, i], rounded_predictions)\n",
    "        cm_plot_labels = ['NO-'+list_classes[i],list_classes[i]]\n",
    "        plot_confusion_matrix(cm, cm_plot_labels, title='Confusion '+list_classes[i]+\" -roc:{:.5f}\".format(scr))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "57aeaa36c47731d6f1790edea897ddccae4ec11d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "4ffb5ba9b22d0d73ded0e5a63dc4c1b50b8c31c3"
   },
   "outputs": [],
   "source": [
    "def ReplaceThreeOrMore(s):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "55c2eea91a124128c83da5fd457c93170ec49b10"
   },
   "outputs": [],
   "source": [
    "def splitstring(s):\n",
    "    # searching the number of characters to split on\n",
    "    proposed_pattern = s[0]\n",
    "    for i, c in enumerate(s[1:], 1):\n",
    "        if c != \" \":\n",
    "            if proposed_pattern == s[i:(i+len(proposed_pattern))]:\n",
    "                # found it\n",
    "                break\n",
    "            else:\n",
    "                proposed_pattern += c\n",
    "    else:\n",
    "        exit(1)\n",
    "\n",
    "    return proposed_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3849a8d7872e7d731f9393d02939649c2fde7590"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text,to_lower=False, rem_urls=False, rem_3plus=False, \\\n",
    "                     split_repeated=False, rem_special=False, rep_num=False,\n",
    "                     man_adj=True, rem_stopwords=False, stem_snowball=False,\\\n",
    "                     stem_porter=False, lemmatize=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    if rem_urls:\n",
    "        text = remove_urls(text)\n",
    "    if to_lower:    \n",
    "        text = text.lower()\n",
    "    if rem_3plus:    \n",
    "        text = ReplaceThreeOrMore(text)\n",
    "\n",
    "    if man_adj: \n",
    "        # Clean the text\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" ! \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "        text = re.sub(r\"\\+\", \" + \", text)\n",
    "        text = re.sub(r\"\\-\", \" - \", text)\n",
    "        text = re.sub(r\"\\=\", \" = \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = re.sub(r\":\", \" : \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        text = re.sub(r\"\\0s\", \"0\", text)\n",
    "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "        text = re.sub(r\"e - mail\", \"email\", text)\n",
    "        text = re.sub(r\"j k\", \"jk\", text)\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # split them into a list\n",
    "    text = text.split()\n",
    "    \n",
    "    if split_repeated:\n",
    "        for i, c in enumerate(text):\n",
    "            text[i]=splitstring(c)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if rem_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    if rem_special: \n",
    "        text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    if rep_num:     \n",
    "        text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_snowball:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    if stem_porter:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "        \n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])   \n",
    " \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if debug:\n",
    "    train_df = pd.read_csv(TRAIN_DATA_FILE)[:1000]\n",
    "    test_df = pd.read_csv(TEST_DATA_FILE)[:200]\n",
    "else:\n",
    "    train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "    test_df = pd.read_csv(TEST_DATA_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "a46196489953296d0dd072027066b8fa681c9207"
   },
   "outputs": [],
   "source": [
    "# Prepare Data \n",
    "list_sentences_train = train_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "list_classes = [\"negative\", \"somewhat negative\", \"neutral\", \"somewhat positive\", \"positive\"]\n",
    "num_classes=5\n",
    "#y = train_df[list_classes].values\n",
    "target=train_df['Rating'].values\n",
    "y1=to_categorical(target)\n",
    "y=np.delete(y1, 0, axis=1)\n",
    "list_sentences_test = test_df[\"ReviewText\"].fillna(\"NA\").values\n",
    "yaux=y[:,[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "135208322b9070ad2928cfe227392e4edb46c99e"
   },
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "6deb22bb9c53566dbcf9009a7a728b62276b35c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b1042778ca4497c696a9b237b2276ba8c4b7d28a"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "a3e5dd1a05067929775036a74bb5bf0cdd364069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672297 train sequences\n",
      "5000 test sequences\n",
      "Average train sequence length: 120\n",
      "Average test sequence length: 239\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, sequences)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, test_sequences)), dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "46cd84528e91ea1e08e6e5dc33fab9f788162837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672297 train sequences\n",
      "5000 test sequences\n",
      "Max train sequence length: 6372\n",
      "Max test sequence length: 3489\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Max train sequence length: {}'.format(np.max(list(map(len, sequences)))))\n",
    "print('Max test sequence length: {}'.format(np.max(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "29be31dfd82202bcb30ede6b9fb8af55028793a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672297 train sequences\n",
      "5000 test sequences\n",
      "Min train sequence length: 0\n",
      "Min test sequence length: 1\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences), 'train sequences')\n",
    "print(len(test_sequences), 'test sequences')\n",
    "print('Min train sequence length: {}'.format(np.min(list(map(len, sequences)))))\n",
    "print('Min test sequence length: {}'.format(np.min(list(map(len, test_sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "bed08c9f1abb024f891225a697ebca50217f6612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 509447 unique tokens\n",
      "Shape of data tensor: (1672297, 300)\n",
      "Shape of label tensor: (1672297, 5)\n",
      "Shape of test_data tensor: (5000, 300)\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "a72e3ea1a3bc19d1297131d4eeae45f138cb8dd9"
   },
   "outputs": [],
   "source": [
    "del comments\n",
    "del test_sequences\n",
    "del sequences\n",
    "del list_sentences_train\n",
    "del list_sentences_test\n",
    "del train_df\n",
    "#del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "5ecd42c012ab948135f36263e72a33b7ac71ee53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing GLOVE-840B vectors\n",
      "Vector ../input/glove840b300dtxt/glove.840B.300d.txt\n",
      "Total 2195896 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index GLOVE  word vectors\n",
    "########################################\n",
    "EMBEDDING_FILE=path+'input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "et=\"GLOVE-840B\"\n",
    "\n",
    "#EMBEDDING_FILE=path+'glove/glove.6B.300d.txt'\n",
    "#et=\"GLOVE-6B\"\n",
    "\n",
    "#EMBEDDING_FILE= path+'prodata/toxic_clean_300d.txt'\n",
    "#et='TOXIC-TXT'\n",
    "\n",
    "#EMBEDDING_FILE= path+'fasttext/crawl-300d-2M.vec'\n",
    "#et=\"FASTTEXT\"\n",
    "\n",
    "\n",
    "print('Indexing '+et+' vectors')\n",
    "print(\"Vector\",EMBEDDING_FILE )\n",
    "#Glove Vectors\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE,  encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    #word = values[0]\n",
    "    #coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "65d5998c9059c7e88c2460fe93fe7c49500f7c09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "## Glove\n",
    "#########\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std\n",
    "\n",
    "########################################\n",
    "## GLOVE prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(max_features, len(word_index))+1\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\n",
    "#embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "nl=0\n",
    "gd=0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        #print ('Over: ',word)\n",
    "        nl = nl +1 \n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        gd=gd+1\n",
    "    else:\n",
    "        #print (word)\n",
    "        nl = nl +1 \n",
    "\n",
    "         \n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "ae765839d81a82ff59ecbfc7fba1dbd2bead9b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix (250001, 300)\n",
      "Tamanho Vocabulario 509447 Maximo de Features 250000\n",
      "Null word embeddings: 391989\n",
      "Good word embeddings: 117458\n"
     ]
    }
   ],
   "source": [
    "#print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0)) \n",
    "print( \"Matrix\", embedding_matrix.shape)\n",
    "print( \"Tamanho Vocabulario\", len(word_index), \"Maximo de Features\",max_features)\n",
    "print('Null word embeddings: %d' % nl)\n",
    "print('Good word embeddings: %d' % gd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "be47095f8aa94db740a624d71490eb8eb17d0e18"
   },
   "outputs": [],
   "source": [
    " def create_model0():\n",
    "    \n",
    "    mdln=\"00-max-d-dr-d-dr-d-dr\"\n",
    "    comment_input = Input((maxlen,))\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    comment_emb = Embedding(max_features, EMBEDDING_DIM, input_length=maxlen, \n",
    "                            embeddings_initializer=\"uniform\")(comment_input)\n",
    "\n",
    "    # we add a GlobalMaxPooling1D, which will extract features from the embeddings\n",
    "    # of all words in the comment\n",
    "    m = GlobalMaxPooling1D()(comment_emb)\n",
    "    d = Dense(1024, activation=act)(m)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(512, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(256, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    d = Dense(128, activation=act)(d)\n",
    "    d = Dropout(rate_drop_dense)(d)\n",
    "    # We project onto a six-unit output layer, and squash it with a sigmoid:\n",
    "    output = Dense(num_classes, activation='sigmoid')(d)\n",
    "\n",
    "    model = Model(inputs=comment_input, outputs=output)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    #for layer in model.layers:\n",
    "     #   weights = layer.get_weights()\n",
    "    #print(weights)\n",
    "    \n",
    "    return model,mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "fd83c1649f01bc7e6834fb0c67d960380c3ea5f3"
   },
   "outputs": [],
   "source": [
    "# ## Modelo LSTM Base Line \n",
    "########################################\n",
    "def create_model1():\n",
    "\n",
    "    mdln=\"01-lstm-att-d-b\"\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=Trainable)\n",
    "\n",
    "    lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n",
    "\n",
    "    comment_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences= embedding_layer(comment_input)\n",
    "    x = lstm_layer(embedded_sequences)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    merged = Attention(maxlen)(x)\n",
    "    merged = Dense(num_dense, activation=act)(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    preds = Dense(num_classes, activation='sigmoid')(merged)\n",
    "\n",
    "    model = Model(inputs=[comment_input], \\\n",
    "            outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "   \n",
    "    return model, mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "ac75b28843070bb7eae9c46dfb1f18ca182d4c9d"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## BASE Model\n",
    "########################################\n",
    "def create_model2( ):\n",
    "\n",
    "    mdln=\"02-base-bilstm-max-dd\"\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=Trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(maxlen,))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Bidirectional(LSTM(num_dense, return_sequences=True, dropout=rate_drop_dense, recurrent_dropout=rate_drop_lstm))(embedded_sequences)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    preds = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "   \n",
    "    return model, mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "d5029667f5d22999704715579c5fb851906695a8"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## Modelo CONV - https://www.kaggle.com/cdubuz/keras-cnn-rnn-0-051-lb\n",
    "########################################\n",
    "\n",
    "def create_model3():\n",
    "\n",
    "        \n",
    "    \n",
    "        mdln=\"03-conv-max-conv-max-gru-d\"\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                EMBEDDING_DIM,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=maxlen,\n",
    "                trainable=Trainable)\n",
    "\n",
    "        sequence_input = Input(shape=(maxlen,))\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        main = Dropout(rate_drop_dense)(embedded_sequences)\n",
    "        main = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')(main)\n",
    "        main = MaxPooling1D(pool_size=2)(main)\n",
    "        main = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')(main)\n",
    "        main = MaxPooling1D(pool_size=2)(main)\n",
    "        main = GRU(64)(main)\n",
    "        main = Dense(32, activation=\"relu\")(main)\n",
    "        main = Dense(num_classes, activation=\"sigmoid\")(main)\n",
    "        model = Model(inputs=sequence_input, outputs=main)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        \n",
    "        return model, mdln\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "85d4d69d3f01099310d1ed8192a0739a0b2e2b1d"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "def create_model4():\n",
    "\n",
    "    mdln=\"04-conv-max-conv-max-bulstm-max-dd\"\n",
    "\n",
    "\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=Trainable)\n",
    "\n",
    "    comment_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    embedded_sequences= embedding_layer(comment_input)\n",
    "    x = Dropout(0.2)(embedded_sequences)\n",
    "    x = Conv1D(filters=EMBEDDING_DIM, kernel_size=4, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters=EMBEDDING_DIM, kernel_size=4, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Bidirectional(LSTM(num_dense, return_sequences=True, dropout=rate_drop_dense, recurrent_dropout=rate_drop_lstm))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    preds = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[comment_input], \\\n",
    "            outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "\n",
    "    return model, mdln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "bcea9c5861fa1a4ff53e572738c50ea8412b7514"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def create_model5():\n",
    "    et='NONE'\n",
    "    mdln=\"05-tridense\"\n",
    "    model = Sequential()\n",
    "    sequence_input = Input(shape=(maxlen,))\n",
    "    # Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "    # in the first layer, you must specify the expected input data shape:\n",
    "    # here, 20-dimensional vectors.\n",
    "    model.add(Dense(64, input_dim=maxlen, init='uniform'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, init='uniform'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, init='uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    return model, mdln\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "3bb72dbbefade3b283dd069b8c80a6e1380a3e7c"
   },
   "outputs": [],
   "source": [
    "def create_model6( ):\n",
    "\n",
    "    mdln=\"06-bicugru-con-max-avg-d\"\n",
    "\n",
    "    input_layer = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n",
    "  \n",
    "\n",
    "    tower_1 = GlobalMaxPool1D()(x)\n",
    "    tower_2 = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    output = concatenate([  tower_1, tower_2])\n",
    "\n",
    "    x = Dense(num_dense, activation=\"relu\")(output)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    preds = Dense(num_classes, activation=\"sigmoid\")(x)                         \n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                 # optimizer=RMSprop(clipvalue=1, clipnorm=1),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "   \n",
    "    return model, mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "2f9cdd7f267188f78e5e90f79123b7babaf1d0d0"
   },
   "outputs": [],
   "source": [
    "def create_model7():\n",
    "    et='NONE'\n",
    "    mdln=\"07-biconv-mas-gru-d\"\n",
    "    embed_size = 256\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    main = Embedding(max_features, embed_size)(inp)\n",
    "    main = Dropout(0.2)(main)\n",
    "    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "    main = MaxPooling1D(pool_size=2)(main)\n",
    "    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "    main = MaxPooling1D(pool_size=2)(main)\n",
    "    main = GRU(32)(main)\n",
    "    main = Dense(16, activation=\"relu\")(main)\n",
    "    main = Dense(num_classes, activation=\"sigmoid\")(main)\n",
    "    model = Model(inputs=inp, outputs=main)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    #model.summary()     \n",
    "    return model, mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "0879a446a38cd4a6476d8912b88d6bc4351e7ced"
   },
   "outputs": [],
   "source": [
    "def create_model8( ):\n",
    "\n",
    "    # PV = 0.9875  (PROC FALSE) / PUBLIC:0.9844\n",
    "    # PV = 0.98587 (PROC TRUE)  / PUBLIC: ?\n",
    "    mdln=\"08-bicugru-max-dd\"\n",
    "\n",
    "\n",
    " #   embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    " #                               weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    \n",
    "    embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=maxlen,\n",
    "        trainable=Trainable)\n",
    "\n",
    "    input_layer = Input(shape=(maxlen,))\n",
    "    embedded_sequences = embedding_layer(input_layer)\n",
    "    x = Bidirectional(CuDNNGRU(num_dense, return_sequences=True))(embedded_sequences)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    preds = Dense(num_classes, activation=\"sigmoid\")(x)                         \n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model, mdln\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "928048206e0c563fc8cc9c457c7605b52f6c98f2"
   },
   "outputs": [],
   "source": [
    "def create_model9( ):\n",
    "\n",
    "        mdln=\"09-bi-lstm-max-dd\"\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                EMBEDDING_DIM,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=maxlen,\n",
    "                trainable=Trainable)\n",
    "\n",
    "        sequence_input = Input(shape=(maxlen,))\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = Bidirectional(LSTM(num_dense, return_sequences=True, dropout=rate_drop_dense, recurrent_dropout=rate_drop_lstm))(embedded_sequences)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(num_dense, activation=\"relu\")(x)\n",
    "        x = Dropout(rate_drop_dense)(x)\n",
    "        x = Dense(num_dense, activation=\"relu\")(x)\n",
    "        x = Dropout(rate_drop_dense)(x)\n",
    "        preds = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "        #print(model.summary())\n",
    "        return model, mdln  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "9009545f78a6435e3342c80f0ee2ed9c6d764c5c"
   },
   "outputs": [],
   "source": [
    "def create_model10( ):\n",
    "\n",
    "    # PV = 0.98734 TRUE / PUBLIC: 0.9852\n",
    "    # PV = 0.98603 FALSE/ PUBLIC: 0.9837\n",
    "    \n",
    "    mdln=\"10-bibi-cugru-dbdd\"\n",
    "\n",
    "\n",
    "    input_layer = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "\n",
    "    main_layer = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n",
    "    main_layer = Dropout(rate_drop_lstm)(main_layer)\n",
    "    main_layer = Bidirectional(CuDNNGRU(num_lstm, return_sequences=False))(main_layer)\n",
    "    main_layer = Dense(num_dense, activation=\"relu\")(main_layer)\n",
    "    main_layer = BatchNormalization()(main_layer)\n",
    "    main_layer = Dense(64, activation=\"relu\")(main_layer)\n",
    "    main_layer = Dropout(rate_drop_dense)(main_layer)\n",
    "    main_layer = Dense(32, activation=\"relu\")(main_layer)\n",
    "    main_layer = Dropout(rate_drop_dense)(main_layer)\n",
    "    preds    = Dense(num_classes, activation=\"sigmoid\")(main_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model, mdln  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "d81dcd2abb6fbd67f0e96a9b84f210517c2fcc10"
   },
   "outputs": [],
   "source": [
    "def create_model11( ):\n",
    "\n",
    "    # PV = 0.98734 TRUE / PUBLIC: 0.9852\n",
    "    # PV = 0.98603 FALSE/ PUBLIC: 0.9837\n",
    "    \n",
    "    mdln=\"11-bicgru-dr-bicgru-dr-d-dr\"\n",
    "\n",
    "    input_layer = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n",
    "    x = Dropout(rate_drop_lstm)(x)\n",
    "    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=False))(x)\n",
    "\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    preds = Dense(num_classes, activation=\"sigmoid\")(x)                         \n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model, mdln    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "4f4e47b63580c81870f6b8444b1aba2ccdd45572"
   },
   "outputs": [],
   "source": [
    "def create_model12():\n",
    "\n",
    "    mdln=\"12-bicgru-con-conv-max-avg-d-dr\"\n",
    "\n",
    "    input_layer = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    tower_1 = Conv1D(filters=num_lstm, kernel_size=2, padding='same', activation='relu')(x)\n",
    "    tower_1 = GlobalMaxPool1D()(tower_1)\n",
    "    tower_2 = Conv1D(filters=num_lstm, kernel_size=2, padding='same', activation='relu')(x)\n",
    "    tower_2 = GlobalAveragePooling1D()(tower_2)\n",
    "    \n",
    "    output = concatenate([  tower_1, tower_2])\n",
    "    out = Dense(num_dense, activation=\"relu\")(output)\n",
    "    out = Dropout(rate_drop_dense)(out)\n",
    "    preds = Dense(num_classes, activation=\"sigmoid\")(out)                         \n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "                       \n",
    "\n",
    "\n",
    "    return model, mdln    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "cee2ac7cca48f36b655430eaaecd6590655ce5b2"
   },
   "outputs": [],
   "source": [
    "# ## Modelo LSTM Base Line \n",
    "########################################\n",
    "def create_model13():\n",
    "        et='NONE'\n",
    "        mdln=\"13-dr-conv-max-conv-max-gru-dr\"\n",
    "        model = Sequential()\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        main = Embedding(max_features, EMBEDDING_DIM)(inp)\n",
    "        main = Dropout(0.2)(main)\n",
    "        main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "        main = MaxPooling1D(pool_size=2)(main)\n",
    "        main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "        main = MaxPooling1D(pool_size=2)(main)\n",
    "        main = GRU(32)(main)\n",
    "        main = Dense(16, activation=\"relu\")(main)\n",
    "        main = Dense(num_classes, activation=\"sigmoid\")(main)\n",
    "        model = Model(inputs=inp, outputs=main)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        return model, mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "17a09f34911294aa8fcb803214b09d57eac3ddd9"
   },
   "outputs": [],
   "source": [
    "def create_model14():\n",
    "\n",
    "        mdln=\"14-culstm-sdr-bn-d-dr\"\n",
    "        \n",
    "        comment_input = Input((maxlen,))\n",
    "\n",
    "        # we start off with an efficient embedding layer which maps\n",
    "        # our vocab indices into embedding_dims dimensions\n",
    "        comment_emb =Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=Trainable)(comment_input)\n",
    "\n",
    "        # we add a GlobalMaxPool1D, which will extract information from the embeddings\n",
    "        # of all words in the document\n",
    "        x = CuDNNLSTM(num_dense, return_sequences=True)(comment_emb)\n",
    "        comment_emb = SpatialDropout1D(0.25)(x)\n",
    "        max_emb = GlobalMaxPool1D()(comment_emb)\n",
    "\n",
    "        # normalized dense layer followed by dropout\n",
    "        main = BatchNormalization()(max_emb)\n",
    "        main = Dense(64)(main)\n",
    "        main = Dropout(0.5)(main)\n",
    "\n",
    "        # We project onto a six-unit output layer, and squash it with sigmoids:\n",
    "        output = Dense(num_classes, activation='sigmoid')(main)\n",
    "\n",
    "        model = Model(inputs=comment_input, outputs=output)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "                                         \n",
    "        \n",
    "        return model, mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "bbc21da20a305e55af6831d9eadeae28acc06b69"
   },
   "outputs": [],
   "source": [
    "def create_model15():\n",
    "        mdln=\"15-sdr-bigru-con-max-avg\"\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=Trainable)(inp)\n",
    "   \n",
    "        x = SpatialDropout1D(rate_drop_dense)(embedding_layer)\n",
    "        x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(x)\n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        max_pool = GlobalMaxPooling1D()(x)        \n",
    "        conc = concatenate([avg_pool, max_pool])\n",
    "        x = Dense(num_dense, activation=\"relu\")(conc)\n",
    "        x = Dropout(rate_drop_dense)(x)\n",
    "        outp = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=outp)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "     \n",
    "        return model, mdln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "b32e43872323a141537924730e1b0cfecc7a6d00"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-9e740b4b92ef>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-9e740b4b92ef>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    return model, mdln\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## BASE Model\n",
    "########################################\n",
    "def create_model16( ):\n",
    "\n",
    "    mdln=\"16-culstm-max-d-dr\"\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=Trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(maxlen,))\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Bidirectional(CuDNNLSTM(num_dense, return_sequences=True))(embedded_sequences)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(num_dense, activation=\"relu\")(x)\n",
    "    x = Dropout(rate_drop_dense)(x)\n",
    "    preds = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer=opt,\n",
    "    \n",
    "    return model, mdln\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "7440ab53eb01889fc75bb04d999fc3116dfe8548"
   },
   "outputs": [],
   "source": [
    "def create_model17( ):\n",
    "    \n",
    "        mdln=\"17-biclstm-max-d-dr-d-dr\"\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                EMBEDDING_DIM,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=maxlen,\n",
    "                trainable=Trainable)\n",
    "\n",
    "        sequence_input = Input(shape=(maxlen,))\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = Bidirectional(CuDNNLSTM(num_dense, return_sequences=True))(embedded_sequences)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(num_dense, activation=\"relu\")(x)\n",
    "        x = Dropout(rate_drop_dense)(x)\n",
    "        x = Dense(num_dense, activation=\"relu\")(x)\n",
    "        x = Dropout(rate_drop_dense)(x)\n",
    "        preds = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "        return model, mdln  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "b3388f5724f8f2adfdf1604142276b754dae1cba"
   },
   "outputs": [],
   "source": [
    "def create_model18( ):\n",
    "    \n",
    "        mdln=\"18-dr-bicgru-max-d-dr\"\n",
    "        embedding_layer = Embedding(nb_words,\n",
    "                EMBEDDING_DIM,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=maxlen,\n",
    "                trainable=Trainable)\n",
    "\n",
    "        sequence_input = Input(shape=(maxlen,))\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = Dropout(rate_drop_dense)(embedded_sequences)\n",
    "        x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(x)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(num_dense, activation=\"relu\")(x)\n",
    "        x = Dropout(rate_drop_dense)(x)\n",
    "        preds = Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "        model = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "        return model, mdln  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "7c538f6cffb4a51aa196167ca613e93b4b519e9e"
   },
   "outputs": [],
   "source": [
    "def create_model19( ):\n",
    "    \n",
    "    \n",
    "        mdln=\"19-sdr-bicgru-con-conv-max-avg-d-dr\"\n",
    "        \n",
    "        input_layer = Input(shape=(maxlen,))\n",
    "        embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                    weights=[embedding_matrix], trainable=Trainable)(input_layer)\n",
    "\n",
    "        x = SpatialDropout1D(0.2)(embedding_layer)\n",
    "        x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(x)\n",
    "\n",
    "        #x = Conv1D(filters=num_lstm, kernel_size=2, padding='same', activation='relu')(x)\n",
    "        x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "        tower_1 = GlobalMaxPool1D()(x)\n",
    "        tower_2 = GlobalAveragePooling1D()(x)\n",
    "\n",
    "        output = concatenate([  tower_1, tower_2])\n",
    "\n",
    "        out = Dense(num_dense, activation=\"relu\")(output)\n",
    "        out = Dropout(rate_drop_dense)(out)\n",
    "        preds = Dense(num_classes, activation=\"sigmoid\")(out)                         \n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=preds)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        return model, mdln    \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "2aa83b488308f4f0c2bafa53ec68f9f2ba9631f0"
   },
   "outputs": [],
   "source": [
    "def dict_to_list(d):\n",
    "    ret = []\n",
    "    for i in d.items():\n",
    "        ret.append(i[1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()\n",
    "\n",
    "def get_validation_predictions(train_data, predictions_valid):\n",
    "    pv = []\n",
    "    for i in range(len(train_data)):\n",
    "        pv.append(predictions_valid[i])\n",
    "    return pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "cf08ffa9fa4c8291c304b4ec4940b15aca7292ca"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "         for v in layer.__dict__:\n",
    "             v_arg = getattr(layer,v)\n",
    "             if (v != \"embeddings\"):\n",
    "                 if hasattr(v_arg,'initializer'):\n",
    "                     initializer_method = getattr(v_arg, 'initializer')\n",
    "                     initializer_method.run(session=session)\n",
    "                     print('reinitializing layer {}.{}'.format(layer.name, v))\n",
    "             else :\n",
    "                  print('keeping layer {}.{}'.format(layer.name, v)) \n",
    "            \n",
    "                        \n",
    "    print (\"reinitializing layers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "44599a485e2d5c1ce2c9f19ba16193f82b6e4399"
   },
   "outputs": [],
   "source": [
    "def evaluate_model ( label, X_train, X_valid, Y_train, Y_valid, STAMP):\n",
    "\n",
    "            modelx=None\n",
    "            modelx,mdln = create_model(label)\n",
    "           \n",
    "\n",
    "            bst_model_path=path+ \"model/\"+\"{val_acc:.5f}-{epoch:02d}-{val_loss:.5f}_\"+ STAMP + '.h5'\n",
    "            #print(bst_model_path)\n",
    "\n",
    "            #early_stopping =EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "            early_stopping =EarlyStopping(monitor=loss, patience=patience, verbose=1)\n",
    "           \n",
    "\n",
    "            model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=False, save_weights_only=False, mode='auto')\n",
    "\n",
    "            callbacks = [ early_stopping, model_checkpoint]\n",
    "\n",
    "            \n",
    "            hist=modelx.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                  shuffle=True, verbose=0, validation_data=(X_valid, Y_valid),\n",
    "                  callbacks=callbacks)\n",
    "            \n",
    "            index_val_loss = hist.history['val_loss'].index(min(hist.history['val_loss']))\n",
    "            bst_model_path=path+ \"model/\"+\"{:.5f}-\".format(hist.history['val_acc'][index_val_loss])+\"{:02d}\".format(index_val_loss+1)+\"-{:.5f}\".format(hist.history['val_loss'][index_val_loss])+\"_\"+STAMP+'.h5'\n",
    " \n",
    "            #reset_weights(model)\n",
    "            K.clear_session()\n",
    "            modelx=None\n",
    "            del modelx\n",
    "            return bst_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "507cde2434fc6819c77d7394cf18c320a38ea0a5"
   },
   "outputs": [],
   "source": [
    "def run_cross_validation_create_models(label,nsplits=10,epochs=3,patience=3):\n",
    "        random_state = 999\n",
    "\n",
    "        yfull_train = dict()\n",
    "        train_full=  []\n",
    "        test_full = []\n",
    "\n",
    "        kf = skf = StratifiedKFold(n_splits=nsplits, shuffle=True, random_state=random_state)\n",
    "        num_fold = 0\n",
    "        sum_score = 0\n",
    "        bestmodel,mdln=create_model(label)\n",
    "        print (Fore.GREEN+\"\\n\"+mdln,\" ***********************************************************\")\n",
    "        print (bestmodel.summary() )\n",
    "        print (Fore.BLACK)\n",
    "        saved_models = []\n",
    "        score = np.zeros(nsplits)\n",
    "        score_partial = np.zeros(nsplits)\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(data,yaux)):\n",
    "            num_fold += 1\n",
    "            print('Fold:',num_fold)\n",
    "\n",
    "            X_train, X_valid = data[train_index],data[test_index]\n",
    "            Y_train, Y_valid = y[train_index], y[test_index]\n",
    "            print('Start KFold number {} from {} - Fitting'.format(num_fold, nsplits))\n",
    "\n",
    "           \n",
    "            bst_model_path=evaluate_model (label,X_train, X_valid, Y_train, Y_valid, STAMP)\n",
    "            \n",
    "          \n",
    "            print (\"Validating\")\n",
    "\n",
    "            ### Getting the Best Model\n",
    "            bestmodel,mdln=create_model(label)\n",
    "            bestmodel.load_weights(bst_model_path)          \n",
    "            \n",
    "            #predictions_valid = bestmodel.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n",
    "            predictions_valid = bestmodel.predict(X_valid, batch_size=batch_size, verbose=0)\n",
    "\n",
    "            score_partial[i] = multi_roc_auc_score(Y_valid, predictions_valid)\n",
    "            print(Fore.BLUE +'Partial Score roc_auc: {:.5f}\\n'.format(score_partial[i])+Fore.BLACK)\n",
    "            info_string = '{:.5f}'.format(score_partial[i]) +\"_fl_\"+'{:02d}'.format(num_fold) +\"_\"+STAMP\n",
    " \n",
    "\n",
    "            train_pred = bestmodel.predict(data, batch_size=batch_size, verbose=1)\n",
    "            score[i] = multi_roc_auc_score(y, train_pred)\n",
    "            print(Fore.GREEN +'Full Score roc_auc: {:.5f}\\n'.format(score[i])+Fore.BLACK)\n",
    "            train_full.append(train_pred)\n",
    "            \n",
    "            test_pred = bestmodel.predict(test_data, batch_size=batch_size, verbose=1)\n",
    "            test_full.append(test_pred)\n",
    "                       \n",
    "            #newfile= path+\"model/\"+\"{:.5f}-roc-\".format(score_partial[i])+bst_model_path[-(len(STAMP)+21):]\n",
    "            newfile= path+\"model/final/\"+\"{:.5f}-roc-\".format(score_partial[i])+bst_model_path[35:]\n",
    "            os.rename(bst_model_path, newfile)\n",
    "            \n",
    "            sum_score += score_partial[i]*len(test_index)\n",
    "\n",
    "            saved_models.append(bestmodel)\n",
    "\n",
    "            del bestmodel\n",
    "        scoreF = sum_score/len(data)\n",
    "        \n",
    "        train_res = np.array( merge_several_folds_mean(train_full, nsplits))  \n",
    "        \n",
    "        print(Fore.RED +'roc_uac train independent: {:.5f}\\n'.format(scoreF))\n",
    "        print (\"Teste Internal Score {:.5f}\".format(score_partial.mean()) )\n",
    "        print (\"Teste External Score {:.5f}\\n\".format(score.mean()) )\n",
    "            \n",
    "        test_res  = merge_several_folds_mean(test_full, nsplits)\n",
    "        results=np.array(test_res)\n",
    "\n",
    "        ratings=np.argmax(results, axis=1)\n",
    "        print(ratings)\n",
    "\n",
    "        submit_df = pd.DataFrame({\"ReviewText\": test_df[\"ReviewText\"], \"Rating\": ratings})\n",
    "        submit_df.to_csv('submission1.csv')\n",
    "        print (\"Submit Rows:\",submit_df[\"Rating\"].count())    \n",
    "            \n",
    "        print(Fore.BLACK)\n",
    "          \n",
    "        K.clear_session()   \n",
    "\n",
    "        return info_string, saved_models \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "a5fc98342442b74b1001e853e6d5175d8a1a8e31"
   },
   "outputs": [],
   "source": [
    "def create_model (label):\n",
    "    \n",
    "            if (label==\"0\"):\n",
    "                return create_model0()\n",
    "            if (label==\"1\"):\n",
    "                return create_model1()\n",
    "            if (label==\"2\"):\n",
    "                return create_model2()\n",
    "            if (label==\"3\"):\n",
    "                return create_model3()            \n",
    "            if (label==\"4\"):\n",
    "                return create_model4()\n",
    "            if (label==\"5\"):\n",
    "                return create_model5()\n",
    "            if (label==\"6\"):\n",
    "                return create_model6()\n",
    "            if (label==\"7\"):\n",
    "                return create_model7()  \n",
    "            if (label==\"8\"):\n",
    "                return create_model8()\n",
    "            if (label==\"9\"):\n",
    "                return create_model9() \n",
    "            if (label==\"10\"):\n",
    "                return create_model10()            \n",
    "            if (label==\"11\"):\n",
    "                return create_model11()                \n",
    "            if (label==\"12\"):\n",
    "                return create_model12()                \n",
    "            if (label==\"13\"):\n",
    "                return create_model13()                \n",
    "            if (label==\"14\"):\n",
    "                return create_model14()                \n",
    "            if (label==\"15\"):\n",
    "                return create_model15()                \n",
    "            if (label==\"16\"):\n",
    "                return create_model16()   \n",
    "            if (label==\"17\"):\n",
    "                return create_model17() \n",
    "            if (label==\"18\"):\n",
    "                return create_model18() \n",
    "            if (label==\"19\"):\n",
    "                return create_model19() \n",
    "            return None,\"None\"\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "d05ad93e4b7e23236978eb12e52a45451f711e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] Started : 25-04-2019 02:25\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_model16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-72543701307c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m\"16\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m              \u001b[0minfo_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_cross_validation_create_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsplits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-f1d5d1a4ca81>\u001b[0m in \u001b[0;36mrun_cross_validation_create_models\u001b[0;34m(label, nsplits, epochs, patience)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnum_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msum_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mbestmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmdln\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmdln\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" ***********************************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbestmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-c8eb438a0227>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_model15\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_model16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"17\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_model17\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model16' is not defined"
     ]
    }
   ],
   "source": [
    "nsplits = 5\n",
    "epochs=5\n",
    "patience=2\n",
    "trainpred=True\n",
    "testpred=True\n",
    "batch_size=512\n",
    "STAMP=\"AA\"\n",
    "best_models= [\"15\",\"16\"]\n",
    "\n",
    "for label in  [\"16\"]:\n",
    "       with timer(label):\n",
    "             info_string, modelos = run_cross_validation_create_models(label, nsplits,epochs, patience)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
